{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d934d768",
   "metadata": {},
   "source": [
    "# 2025.06.05. (THU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94c812",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937bd760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.25'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee0c84",
   "metadata": {},
   "source": [
    "### OpenAI Model\n",
    "\n",
    "* https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f27791",
   "metadata": {},
   "source": [
    "### OpenAI LLMs\n",
    "\n",
    "* [LLMs](https://platform.openai.com/docs/models)\n",
    "\n",
    "* [Pricing](https://platform.openai.com/docs/pricing)\n",
    "\n",
    "* [Token Size Confirmation](https://platform.openai.com/tokenizer)\n",
    "\n",
    "    * 1 Token: About 3~4 English letters. About 1~2 Korean letters\n",
    "\n",
    "    * As models continue updating, token size is getting larger as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b602ccd",
   "metadata": {},
   "source": [
    "### Get API w/ OpenAI Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b9363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "flag = load_dotenv()\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c378b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()  # if saved as \"OPENAI_API_KEY\" in environment variable, be able to omit api key input.\n",
    "message = [\n",
    "    {\n",
    "        \"role\":\"user\",   # user, assistant, system\n",
    "        \"content\":\"OpenAI의 LLM 모델은 무엇이 있나요?\"   # prompt\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',    # select a model\n",
    "    messages = message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df71f60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bf70vErBgUihvSSsIIhUxSv7OGxFW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전이 있습니다. 주로 알려진 모델은 다음과 같습니다:\\n\\n1. **GPT-2**: OpenAI의 두 번째 버전의 생성적 사전 훈련 변환기(Generative Pre-trained Transformer)로, 자연어 이해 및 생성 작업에 탁월한 성능을 보여줍니다.\\n\\n2. **GPT-3**: GPT-2의 후속 모델로, 1750억 개의 매개변수를 가지고 있어 더욱 뛰어난 성능과 다양한 작업에 대한 능력을 제공합니다. GPT-3는 많은 애플리케이션에서 사용되고 있습니다.\\n\\n3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 성능을 발휘하며 사용자 경험이 향상되었습니다.\\n\\n4. **GPT-4**: 최신 모델로, 이전 모델들보다 더 향상된 성능을 제공합니다. GPT-4는 다양한 맥락을 이해하고 복잡한 작업을 수행하는 데 강력합니다.\\n\\n이 외에도 특정 애플리케이션에 맞춘 다양한 모델이 있을 수 있습니다. OpenAI는 지속적으로 모델을 개선하고 새로운 버전을 개발하고 있습니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749138109, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=261, prompt_tokens=19, total_tokens=280, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca82138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전이 있습니다. 주로 알려진 모델은 다음과 같습니다:\n",
      "\n",
      "1. **GPT-2**: OpenAI의 두 번째 버전의 생성적 사전 훈련 변환기(Generative Pre-trained Transformer)로, 자연어 이해 및 생성 작업에 탁월한 성능을 보여줍니다.\n",
      "\n",
      "2. **GPT-3**: GPT-2의 후속 모델로, 1750억 개의 매개변수를 가지고 있어 더욱 뛰어난 성능과 다양한 작업에 대한 능력을 제공합니다. GPT-3는 많은 애플리케이션에서 사용되고 있습니다.\n",
      "\n",
      "3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 성능을 발휘하며 사용자 경험이 향상되었습니다.\n",
      "\n",
      "4. **GPT-4**: 최신 모델로, 이전 모델들보다 더 향상된 성능을 제공합니다. GPT-4는 다양한 맥락을 이해하고 복잡한 작업을 수행하는 데 강력합니다.\n",
      "\n",
      "이 외에도 특정 애플리케이션에 맞춘 다양한 모델이 있을 수 있습니다. OpenAI는 지속적으로 모델을 개선하고 새로운 버전을 개발하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e730d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전이 있습니다. 주로 알려진 모델은 다음과 같습니다:\n",
       "\n",
       "1. **GPT-2**: OpenAI의 두 번째 버전의 생성적 사전 훈련 변환기(Generative Pre-trained Transformer)로, 자연어 이해 및 생성 작업에 탁월한 성능을 보여줍니다.\n",
       "\n",
       "2. **GPT-3**: GPT-2의 후속 모델로, 1750억 개의 매개변수를 가지고 있어 더욱 뛰어난 성능과 다양한 작업에 대한 능력을 제공합니다. GPT-3는 많은 애플리케이션에서 사용되고 있습니다.\n",
       "\n",
       "3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 성능을 발휘하며 사용자 경험이 향상되었습니다.\n",
       "\n",
       "4. **GPT-4**: 최신 모델로, 이전 모델들보다 더 향상된 성능을 제공합니다. GPT-4는 다양한 맥락을 이해하고 복잡한 작업을 수행하는 데 강력합니다.\n",
       "\n",
       "이 외에도 특정 애플리케이션에 맞춘 다양한 모델이 있을 수 있습니다. OpenAI는 지속적으로 모델을 개선하고 새로운 버전을 개발하고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ec02d",
   "metadata": {},
   "source": [
    "### OpenAI API w/ Langchain\n",
    "\n",
    "* **ChatOpenAI**\n",
    "\n",
    "    * chat-based model\n",
    "\n",
    "    * default: gpt-3.5-turbo\n",
    "\n",
    "    * type of I/O : `Message`\n",
    "\n",
    "* **OpenAI**\n",
    "\n",
    "    * text completion model\n",
    "\n",
    "    * default: gpt-3.5-turbo-instruct\n",
    "    \n",
    "        * instruct model only\n",
    "\n",
    "    * type of I/O: `str`\n",
    "\n",
    "* Params of Initializer\n",
    "\n",
    "    * `temperature`\n",
    "\n",
    "        * sampling temperature(randomness & creativity)\n",
    "\n",
    "        * 0 ~ 2(float). The greater -> The more random the output becomes.\n",
    "\n",
    "        * Large value for creative activities. Small value for precise responses.\n",
    "\n",
    "    * `model_name`\n",
    "\n",
    "    * `max_tokens`: maximum number of output tokens \n",
    "\n",
    "    * `api_key`\n",
    "    \n",
    "        * when putting API Key directly\n",
    "\n",
    "        * if in environment variables, omit.\n",
    "\n",
    "* Method\n",
    "\n",
    "    * `invoke(message)`: return response of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcdddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"OpenAI LLM 모델 종류를 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8404d56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI는 여러 종류의 대형 언어 모델(LLM)을 개발했습니다. 다음은 주요 모델들의 목록입니다:\\n\\n1. **GPT (Generative Pre-trained Transformer)**: 이 모델은 자연어 처리에 사용되는 기본적인 구조로, 다양한 버전이 존재합니다.\\n   - **GPT-1**: 최초의 모델로, 기본적인 아이디어와 구조를 시연했습니다.\\n   - **GPT-2**: 더 큰 데이터셋으로 훈련되어, 다양한 자연어 처리 작업에서 뛰어난 성능을 보였습니다. 초기에는 완전한 모델의 공개를 주저했지만, 이후 전체 모델이 공개되었습니다.\\n   - **GPT-3**: 이전 모델보다 훨씬 더 큰 규모(1750억 개의 매개변수)를 가지고 있으며, 다양한 작업에서 강력한 성능을 보입니다.\\n\\n2. **ChatGPT**: 사용자와의 대화에 최적화된 GPT-3 기반 모델로, 대화형 AI로 많이 활용되고 있습니다. 특히, GPT-3.5와 GPT-4 버전이 있으며, 최신 기능과 향상된 성능을 제공합니다.\\n\\n3. **Codex**: 프로그래밍 코드를 이해하고 생성하는 데 특화된 모델로, GitHub Copilot 등의 도구에 사용됩니다. 주로 코드 관련 질문이나 작업에 응답하는 데 도움을 줍니다.\\n\\n4. **DALL-E**: 텍스트 설명을 기반으로 이미지를 생성하는 모델로, 이미지 생성 분야에 혁신을 가져왔습니다. DALL-E 2와 같은 후속 모델들은 품질과 다양성을 개선했습니다.\\n\\n5. **CLIP (Contrastive Language–Image Pre-training)**: 이미지와 텍스트 간의 연관성을 학습하는 모델로, 이미지를 텍스트 설명과 연결짓는 데 사용됩니다.\\n\\n각 모델은 특정 용도에 맞춰 최적화되어 있으며, OpenAI는 지속적으로 새로운 모델과 기술을 개발하고 업데이트하고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 421, 'prompt_tokens': 17, 'total_tokens': 438, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BfhmJR5ZV1miN5wmoTXDPe5DFrIFK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--51450f22-411b-428d-aac2-3e614a1cb080-0', usage_metadata={'input_tokens': 17, 'output_tokens': 421, 'total_tokens': 438, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1c51a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI는 여러 종류의 대형 언어 모델(LLM)을 개발했습니다. 다음은 주요 모델들의 목록입니다:\n",
      "\n",
      "1. **GPT (Generative Pre-trained Transformer)**: 이 모델은 자연어 처리에 사용되는 기본적인 구조로, 다양한 버전이 존재합니다.\n",
      "   - **GPT-1**: 최초의 모델로, 기본적인 아이디어와 구조를 시연했습니다.\n",
      "   - **GPT-2**: 더 큰 데이터셋으로 훈련되어, 다양한 자연어 처리 작업에서 뛰어난 성능을 보였습니다. 초기에는 완전한 모델의 공개를 주저했지만, 이후 전체 모델이 공개되었습니다.\n",
      "   - **GPT-3**: 이전 모델보다 훨씬 더 큰 규모(1750억 개의 매개변수)를 가지고 있으며, 다양한 작업에서 강력한 성능을 보입니다.\n",
      "\n",
      "2. **ChatGPT**: 사용자와의 대화에 최적화된 GPT-3 기반 모델로, 대화형 AI로 많이 활용되고 있습니다. 특히, GPT-3.5와 GPT-4 버전이 있으며, 최신 기능과 향상된 성능을 제공합니다.\n",
      "\n",
      "3. **Codex**: 프로그래밍 코드를 이해하고 생성하는 데 특화된 모델로, GitHub Copilot 등의 도구에 사용됩니다. 주로 코드 관련 질문이나 작업에 응답하는 데 도움을 줍니다.\n",
      "\n",
      "4. **DALL-E**: 텍스트 설명을 기반으로 이미지를 생성하는 모델로, 이미지 생성 분야에 혁신을 가져왔습니다. DALL-E 2와 같은 후속 모델들은 품질과 다양성을 개선했습니다.\n",
      "\n",
      "5. **CLIP (Contrastive Language–Image Pre-training)**: 이미지와 텍스트 간의 연관성을 학습하는 모델로, 이미지를 텍스트 설명과 연결짓는 데 사용됩니다.\n",
      "\n",
      "각 모델은 특정 용도에 맞춰 최적화되어 있으며, OpenAI는 지속적으로 새로운 모델과 기술을 개발하고 업데이트하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7d4748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='저는 AI 언어 모델이라 특별한 이름은 없지만, 원하는 이름으로 불러주셔도 됩니다! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 13, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bfhn76x2c6mH1eGDLRrnL7qIASQCY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ecae08f1-6f10-4282-975c-5a87915ef4a8-0', usage_metadata={'input_tokens': 13, 'output_tokens': 31, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"너 이름이 뭐니?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8941e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd0ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "<답변형식>\n",
    "목록형식으로 답변해줘.\n",
    "- 한글이름(영어이름): 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbdee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양계에서 가장 작은 행성이자 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 크다.\n",
      "- 금성(Venus): 지구와 유사한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생하여 매우 높은 온도를 유지한다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 다양한 생태계와 물이 존재하는 것이 특징이다.\n",
      "- 화성(Mars): '붉은 행성'으로 알려져 있으며, 탐사와 연구가 많이 이루어진 행성이다. 수명체가 있는 과거의 증거가 발견되기도 했다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성이며, 거대한 가스 행성으로 수많은 위성을 가진다. 대표적으로 유로파와 가니메데가 있다.\n",
      "- 토성(Saturn): 아름다운 고리 시스템을 가진 가스 행성으로, 목성보다 크기는 작지만 여전히 태양계에서 두 번째로 큰 행성이다.\n",
      "- 천왕성(Uranus): 옆으로 누워 있는 회전축을 가진 독특한 행성으로, 청록색의 대기와 수많은 고리 및 위성을 가지고 있다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 바깥쪽에 위치한 행성으로, 푸른 색깔과 강한 바람이 특징적이다. 수소, 헬륨, 메탄으로 구성된 대기를 가졌다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)   # default temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f3f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양계에서 가장 가까운 행성으로, 매우 높은 온도와 낮은 온도를 가진 극단적인 기후를 지니고 있습니다.\n",
      "- 금성(Venus): 태양계에서 두 번째로 가까운 행성으로, 두꺼운 대기와 강한 온실 효과로 인해 매우 뜨거운 환경을 가지고 있습니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 가지고 있습니다.\n",
      "- 화성(Mars): 붉은 색의 행성으로 알려져 있으며, 과거에 물이 있었던 증거가 있는 유망한 탐사 대상입니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 거대한 가스행성이며 강력한 자기장이 있습니다.\n",
      "- 토성(Saturn): 고리로 유명한 행성으로, 목성과 마찬가지로 가스행성입니다.\n",
      "- 천왕성(Uranus): 독특하게도 옆으로 기울어져 있는 축을 가진 행성이며, 블루-그린 색상을 띱니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 바람이 매우 강하고 푸른 색을 띠고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)   # temperature: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d38959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatting format - w/ role\n",
    "prompt = [\n",
    "    ('user', '막걸리 제조법을 알려줘.')   # ('role', 'message')\n",
    "]\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b276b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "막걸리는 한국의 전통 발효주로, 쌀, 물, 누룩을 주재료로 사용하여 만듭니다. 아래는 간단한 막걸리 제조법입니다.\n",
      "\n",
      "### 재료\n",
      "1. 쌀 2컵 (약 400g)\n",
      "2. 물 4컵 (약 1,000ml)\n",
      "3. 누룩 2~3큰술 (약 30g)\n",
      "4. 설탕 (선택사항, 단맛을 원할 경우)\n",
      "\n",
      "### 제조 과정\n",
      "1. **쌀 세척**: 쌀을 깨끗이 씻어 물에 담가 4시간 이상 불립니다. 쌀이 충분히 불면 물기를 제거합니다.\n",
      "\n",
      "2. **쌀 찌기**: 불린 쌀을 찜통에 넣고 약 30분간 찝니다. 쌀이 고루 익도록 중간중간 섞어주는 것이 좋습니다. 익힌 쌀은 식혀둡니다.\n",
      "\n",
      "3. **발효 준비**: 찐 쌀이 식으면, 누룩을 잘게 부수어 쌀과 섞습니다. 이때 설탕을 추가하고 싶은 경우 함께 넣습니다.\n",
      "\n",
      "4. **혼합**: 준비한 물 4컵을 쌀과 누룩 혼합물에 천천히 부어가며 골고루 섞어줍니다.\n",
      "\n",
      "5. **발효**: 혼합물을 깨끗한 유리병이나 fermenter에 옮기고 뚜껑을 덮되 완전히 밀폐하지 않고 공기가 통할 수 있도록 합니다. 상온에서 약 3~5일간 발효를 시킵니다. 이때 가끔 저어주면 좋습니다.\n",
      "\n",
      "6. **완성 및 필터링**: 발효가 완료되면 거름망이나 천을 사용해 걸러줍니다. 이때 나오는 막걸리가 바로 마실 수 있는 막걸리입니다.\n",
      "\n",
      "7. **보관**: 최종적으로 걸러낸 막걸리는 냉장고에 보관하여 차갑게 해서 마시면 좋습니다.\n",
      "\n",
      "### 주의사항\n",
      "- 위생 상태를 잘 유지해야 합니다. 모든 도구와 용기는 깨끗이 소독하여 사용하세요.\n",
      "- 발효 기간에 따라 맛이 달라질 수 있으니 기호에 맞게 조절하세요.\n",
      "\n",
      "이렇게 만든 막걸리는 온도와 발효 시간에 따라 다르게 즐길 수 있습니다. 맛있게 만들어 보세요!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76f42738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 540,\n",
       "  'prompt_tokens': 16,\n",
       "  'total_tokens': 556,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_34a54ae93c',\n",
       " 'id': 'chatcmpl-BfhqXBNUdrCQRrafJbfsAcelbMsHt',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de26c1c",
   "metadata": {},
   "source": [
    "### Using Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a174c",
   "metadata": {},
   "source": [
    "#### Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9f9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4399393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75427e2a930471a86ce65eb20338d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\98_ja\\AppData\\Local\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\98_ja\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4598b0fe3224d78bb9c885bd76a4ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa766adc16148c7b134530af7d5dbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5797b9bb34754ac8a7f106d856a33a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aca012d1991454aa1611656ef21ec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae69be0891f24a3cad655161bbd3fb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babafc34e0e14fb38562c762c348be32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1d34eae3854752a39758def4830f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = 'google/gemma-3-1b-it' # 1 billion params, instruction training\n",
    "\n",
    "model_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id = model_id,\n",
    "    task = 'text-generation',\n",
    "    pipeline_kwargs = {\"max_new_tokens\":50}  # setting for tansformers.pipeline()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb155c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_hf.invoke('한국의 수도는 어디인가요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268b0a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 어디인가요?\\n\\n정답은 서울입니다.\\n\\n---\\n\\n답변이 마음에 드셨나요? 😊\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f060a",
   "metadata": {},
   "source": [
    "### [Anthropic](https://www.anthropic.com/) - [Claude](https://claude.ai/login?returnTo=%2F%3F)\n",
    "\n",
    "* **Haiku**, **Sonnet**, **Opus**\n",
    "\n",
    "* [Pricing](https://docs.anthropic.com/en/docs/about-claude/pricing)\n",
    "\n",
    "* [Utilizing by Langchain](https://python.langchain.com/docs/integrations/chat/anthropic/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d50958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = \"claude-sonnet-4-20250514\"\n",
    "llm = ChatAnthropic(\n",
    "    model = model,\n",
    "    temperature = 0.2,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2118ab",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    ": a platform which helps to easily run open-source LLMs **on a local environment**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a67e8b",
   "metadata": {},
   "source": [
    "**Features**\n",
    "\n",
    "* Various Models: Llama 3, Mistral, Phi 3, etc.\n",
    "\n",
    "* Convenience: to install and run models by simple commands\n",
    "\n",
    "* OS compatibility: Mac, Windows, Linux, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b16b27",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "\n",
    "* Fit your OS [here](https://ollama.com/download)\n",
    "\n",
    "* Windows: nothing but following the presets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef55f3a",
   "metadata": {},
   "source": [
    "#### Models\n",
    "\n",
    "* https://ollama.com/search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3c00c",
   "metadata": {},
   "source": [
    "#### Commands\n",
    "\n",
    "* `ollama pull MODEL_NAME`\n",
    ": not a run but just an installation\n",
    "\n",
    "* `ollama run MODEL_NAME`\n",
    "\n",
    "    * run\n",
    "\n",
    "    * to install if it's the first run\n",
    "\n",
    "    * when putting `PROMPT` on a terminal, we can get a response of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d547224",
   "metadata": {},
   "source": [
    "#### Python/Langchain API\n",
    "\n",
    "* ollama api\n",
    "\n",
    "    * https://github.com/ollama/ollama-python\n",
    "\n",
    "* langchain-ollama\n",
    "\n",
    "    * https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "\n",
    "* Installation\n",
    "\n",
    "    * `pip install langchain-ollama`\n",
    "\n",
    "    * `ollama` package auto-installed as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2a906",
   "metadata": {},
   "source": [
    "* cf. GUI: open-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a93a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model = 'qwen3:0.6b')\n",
    "response = model.invoke('대한민국의 수도의 이름은?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e609f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the name of the capital of the Republic of Korea. I need to recall the correct answer. I remember that the capital is called Seoul. Let me double-check to make sure there's no other city with the same name. No, Seoul is the capital. Also, I should mention that it's a直辖市 (a city-state) so it's part of the larger South Korean territory. Wait, does it have a different name in some other language? No, the official name in Korean is Seoul. So the answer is definitely Seoul.\n",
      "</think>\n",
      "\n",
      "대한민국의 수도는 **서울 (Seoul)**입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac2b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking for the name of the Korean capital. Let me think. I know that Korea's capital is Seoul. But wait, is there a different name? I remember that in some contexts, like in a game or a specific country, they might refer to it differently. Wait, no, the official name is Seoul. But maybe in some official documents, they use another term. Let me check my knowledge. Oh right, in the Korean government's official documents, they also use Seoul as the capital. So I think the answer is Seoul. But just to be sure, I should confirm. Yes, Seoul is the capital of the Democratic People's Republic of Korea, which is now known as South Korea. So the name is definitely Seoul.\n",
      "</think>\n",
      "\n",
      "The Korean capital is **Seoul**.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"What is name of Korean capital?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f884a4",
   "metadata": {},
   "source": [
    "### Gemini\n",
    "\n",
    "* [Model](https://ai.google.dev/gemini-api/docs/models?hl=ko)\n",
    "\n",
    "* [Pricing](https://ai.google.dev/gemini-api/docs/pricing?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e69a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16314bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model = 'gemini-2.5-flash-preview-05-20'\n",
    ")\n",
    "\n",
    "response = model.invoke('gemini와 gemma의 차이는?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f04806e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, 그 목적, 규모, 접근 방식에서 중요한 차이가 있습니다.\\n\\n핵심적인 차이를 한 문장으로 요약하자면:\\n\\n*   **Gemini (제미니)**는 Google의 **가장 강력하고 범용적인 플래그십 AI 모델**이며, 주로 Google 제품 및 서비스(Bard/Gemini 챗봇, Google Cloud 등)를 통해 **폐쇄적으로 제공**됩니다.\\n*   **Gemma (젬마)**는 Gemini의 연구를 기반으로 개발된 **경량의 오픈 소스 AI 모델 패밀리**로, 개발자와 연구자들이 **자유롭게 다운로드하여 사용하고 커스터마이징**할 수 있도록 설계되었습니다.\\n\\n다음은 더 자세한 비교입니다.\\n\\n---\\n\\n### Gemini (제미니)\\n\\n*   **목적 및 역할:** Google의 최첨단 기술을 대표하는 **플래그십 AI 모델**입니다. 복잡한 추론, 다중 모드(텍스트, 이미지, 오디오, 비디오) 이해 및 생성, 코딩, 창의적 작업 등 광범위한 고성능 작업을 수행하는 데 중점을 둡니다.\\n*   **규모 및 성능:**\\n    *   **매우 크고 강력합니다.** (Ultra, Pro, Nano 등 다양한 크기가 있지만, 전반적으로 Gemma보다 훨씬 큽니다.)\\n    *   **다중 모드(Multimodal):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 생성할 수 있습니다.\\n    *   **최고 수준의 성능:** 현존하는 AI 모델 중 가장 뛰어난 성능을 자랑하는 모델 중 하나입니다.\\n*   **접근성:**\\n    *   **폐쇄형/API 기반:** 직접 모델 파일을 다운로드하여 실행할 수 없습니다. Google의 서비스(예: Bard/Gemini 챗봇), Google Cloud Vertex AI, Google AI Studio 등의 API를 통해 접근하고 사용합니다.\\n    *   **Google 제품 통합:** Google 검색, Workspace (Duet AI), Android 등 Google의 다양한 제품과 서비스에 통합되어 사용자 경험을 향상시킵니다.\\n*   **대상:** 일반 사용자, 엔터프라이즈 고객, 복잡한 AI 솔루션을 구축하려는 개발자.\\n\\n---\\n\\n### Gemma (젬마)\\n\\n*   **목적 및 역할:** Gemini 개발에 사용된 연구와 기술을 기반으로 만들어진 **오픈 소스 모델 패밀리**입니다. 개발자와 연구자들이 AI 혁신을 가속화하고, 자신만의 AI 애플리케이션을 구축하며, 모델을 커스터마이징하고 연구할 수 있도록 지원하는 것이 주된 목적입니다.\\n*   **규모 및 성능:**\\n    *   **경량(Lightweight):** Gemini에 비해 훨씬 작고 효율적입니다. (예: 2B, 7B 파라미터 모델, 이후 9B, 27B 등 다양한 크기 출시)\\n    *   **텍스트 기반:** 주로 텍스트 생성 및 이해에 특화되어 있습니다 (초기 버전 기준).\\n    *   **효율성:** 제한된 컴퓨팅 자원에서도 실행될 수 있도록 최적화되어 있습니다.\\n*   **접근성:**\\n    *   **오픈 소스:** 모델 가중치와 코드를 공개하여 누구나 다운로드하여 사용하고 수정하며 재배포할 수 있습니다 (특정 사용 약관 하에).\\n    *   **로컬 실행 및 미세 조정(Fine-tuning):** 개발자들이 자신의 서버, 심지어 일부 개인 장치에서도 모델을 실행하고 특정 작업에 맞게 미세 조정할 수 있습니다.\\n*   **대상:** AI 개발자, 연구자, 학생, 스타트업, 제한된 자원으로 AI 솔루션을 구축하려는 사람들.\\n\\n---\\n\\n### 주요 차이점 요약\\n\\n| 특징           | Gemini (제미니)                                   | Gemma (젬마)                                      |\\n| :------------- | :------------------------------------------------ | :------------------------------------------------ |\\n| **성격**       | Google의 플래그십, 최첨단 AI 모델 (폐쇄형)       | Google의 오픈 소스 경량 AI 모델 패밀리          |\\n| **접근 방식**  | API, Google 제품/서비스를 통해 접근               | 모델 파일 다운로드, 로컬 실행, 커스터마이징 가능 |\\n| **규모/성능**  | 매우 크고 강력함, 다중 모드, 최고 수준의 성능     | 작고 효율적, 주로 텍스트 기반, 효율성 중시       |\\n| **주요 용도**  | 복잡한 AI 작업, Google 제품 경험 향상             | 맞춤형 AI 앱 개발, 연구, 로컬 배포, 미세 조정    |\\n| **타겟 사용자** | 일반 사용자, 엔터프라이즈, 고성능 AI 솔루션 개발자 | AI 개발자, 연구자, 학생, 소규모 프로젝트         |\\n| **관계**       | 핵심 기술, 기반                                   | Gemini 연구에서 파생된 오픈 소스 버전             |\\n\\n---\\n\\n**간단한 비유:**\\n\\n*   **Gemini**는 **Google이 직접 운영하는 최고급 레스토랑의 시그니처 요리**와 같습니다. 당신은 그 요리를 맛볼 수는 있지만, 레시피나 주방 장비를 직접 사용할 수는 없습니다.\\n*   **Gemma**는 **그 최고급 레스토랑의 셰프가 핵심 레시피를 바탕으로 집에서 쉽게 만들 수 있도록 공개한 간소화된 버전의 레시피 키트**와 같습니다. 당신은 그 키트를 사서 직접 요리하고, 원하는 대로 재료를 추가하거나 바꿔볼 수 있습니다.\\n\\n결론적으로, Gemini는 Google의 AI 역량을 보여주는 가장 강력한 결과물이며, Gemma는 그 역량을 외부 개발자들이 활용하고 확장할 수 있도록 문을 연 모델입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'models/gemini-2.5-flash-preview-05-20', 'safety_ratings': []}, id='run--72c4d279-9928-4560-b583-29f368405d36-0', usage_metadata={'input_tokens': 10, 'output_tokens': 1285, 'total_tokens': 2413, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1118}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d6c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, 그 목적, 규모, 접근 방식에서 중요한 차이가 있습니다.\n",
      "\n",
      "핵심적인 차이를 한 문장으로 요약하자면:\n",
      "\n",
      "*   **Gemini (제미니)**는 Google의 **가장 강력하고 범용적인 플래그십 AI 모델**이며, 주로 Google 제품 및 서비스(Bard/Gemini 챗봇, Google Cloud 등)를 통해 **폐쇄적으로 제공**됩니다.\n",
      "*   **Gemma (젬마)**는 Gemini의 연구를 기반으로 개발된 **경량의 오픈 소스 AI 모델 패밀리**로, 개발자와 연구자들이 **자유롭게 다운로드하여 사용하고 커스터마이징**할 수 있도록 설계되었습니다.\n",
      "\n",
      "다음은 더 자세한 비교입니다.\n",
      "\n",
      "---\n",
      "\n",
      "### Gemini (제미니)\n",
      "\n",
      "*   **목적 및 역할:** Google의 최첨단 기술을 대표하는 **플래그십 AI 모델**입니다. 복잡한 추론, 다중 모드(텍스트, 이미지, 오디오, 비디오) 이해 및 생성, 코딩, 창의적 작업 등 광범위한 고성능 작업을 수행하는 데 중점을 둡니다.\n",
      "*   **규모 및 성능:**\n",
      "    *   **매우 크고 강력합니다.** (Ultra, Pro, Nano 등 다양한 크기가 있지만, 전반적으로 Gemma보다 훨씬 큽니다.)\n",
      "    *   **다중 모드(Multimodal):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 생성할 수 있습니다.\n",
      "    *   **최고 수준의 성능:** 현존하는 AI 모델 중 가장 뛰어난 성능을 자랑하는 모델 중 하나입니다.\n",
      "*   **접근성:**\n",
      "    *   **폐쇄형/API 기반:** 직접 모델 파일을 다운로드하여 실행할 수 없습니다. Google의 서비스(예: Bard/Gemini 챗봇), Google Cloud Vertex AI, Google AI Studio 등의 API를 통해 접근하고 사용합니다.\n",
      "    *   **Google 제품 통합:** Google 검색, Workspace (Duet AI), Android 등 Google의 다양한 제품과 서비스에 통합되어 사용자 경험을 향상시킵니다.\n",
      "*   **대상:** 일반 사용자, 엔터프라이즈 고객, 복잡한 AI 솔루션을 구축하려는 개발자.\n",
      "\n",
      "---\n",
      "\n",
      "### Gemma (젬마)\n",
      "\n",
      "*   **목적 및 역할:** Gemini 개발에 사용된 연구와 기술을 기반으로 만들어진 **오픈 소스 모델 패밀리**입니다. 개발자와 연구자들이 AI 혁신을 가속화하고, 자신만의 AI 애플리케이션을 구축하며, 모델을 커스터마이징하고 연구할 수 있도록 지원하는 것이 주된 목적입니다.\n",
      "*   **규모 및 성능:**\n",
      "    *   **경량(Lightweight):** Gemini에 비해 훨씬 작고 효율적입니다. (예: 2B, 7B 파라미터 모델, 이후 9B, 27B 등 다양한 크기 출시)\n",
      "    *   **텍스트 기반:** 주로 텍스트 생성 및 이해에 특화되어 있습니다 (초기 버전 기준).\n",
      "    *   **효율성:** 제한된 컴퓨팅 자원에서도 실행될 수 있도록 최적화되어 있습니다.\n",
      "*   **접근성:**\n",
      "    *   **오픈 소스:** 모델 가중치와 코드를 공개하여 누구나 다운로드하여 사용하고 수정하며 재배포할 수 있습니다 (특정 사용 약관 하에).\n",
      "    *   **로컬 실행 및 미세 조정(Fine-tuning):** 개발자들이 자신의 서버, 심지어 일부 개인 장치에서도 모델을 실행하고 특정 작업에 맞게 미세 조정할 수 있습니다.\n",
      "*   **대상:** AI 개발자, 연구자, 학생, 스타트업, 제한된 자원으로 AI 솔루션을 구축하려는 사람들.\n",
      "\n",
      "---\n",
      "\n",
      "### 주요 차이점 요약\n",
      "\n",
      "| 특징           | Gemini (제미니)                                   | Gemma (젬마)                                      |\n",
      "| :------------- | :------------------------------------------------ | :------------------------------------------------ |\n",
      "| **성격**       | Google의 플래그십, 최첨단 AI 모델 (폐쇄형)       | Google의 오픈 소스 경량 AI 모델 패밀리          |\n",
      "| **접근 방식**  | API, Google 제품/서비스를 통해 접근               | 모델 파일 다운로드, 로컬 실행, 커스터마이징 가능 |\n",
      "| **규모/성능**  | 매우 크고 강력함, 다중 모드, 최고 수준의 성능     | 작고 효율적, 주로 텍스트 기반, 효율성 중시       |\n",
      "| **주요 용도**  | 복잡한 AI 작업, Google 제품 경험 향상             | 맞춤형 AI 앱 개발, 연구, 로컬 배포, 미세 조정    |\n",
      "| **타겟 사용자** | 일반 사용자, 엔터프라이즈, 고성능 AI 솔루션 개발자 | AI 개발자, 연구자, 학생, 소규모 프로젝트         |\n",
      "| **관계**       | 핵심 기술, 기반                                   | Gemini 연구에서 파생된 오픈 소스 버전             |\n",
      "\n",
      "---\n",
      "\n",
      "**간단한 비유:**\n",
      "\n",
      "*   **Gemini**는 **Google이 직접 운영하는 최고급 레스토랑의 시그니처 요리**와 같습니다. 당신은 그 요리를 맛볼 수는 있지만, 레시피나 주방 장비를 직접 사용할 수는 없습니다.\n",
      "*   **Gemma**는 **그 최고급 레스토랑의 셰프가 핵심 레시피를 바탕으로 집에서 쉽게 만들 수 있도록 공개한 간소화된 버전의 레시피 키트**와 같습니다. 당신은 그 키트를 사서 직접 요리하고, 원하는 대로 재료를 추가하거나 바꿔볼 수 있습니다.\n",
      "\n",
      "결론적으로, Gemini는 Google의 AI 역량을 보여주는 가장 강력한 결과물이며, Gemma는 그 역량을 외부 개발자들이 활용하고 확장할 수 있도록 문을 연 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76d234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google에서 개발한 AI 모델이지만, 그 목적, 규모, 접근 방식에서 중요한 차이가 있습니다.\n",
       "\n",
       "핵심적인 차이를 한 문장으로 요약하자면:\n",
       "\n",
       "*   **Gemini (제미니)**는 Google의 **가장 강력하고 범용적인 플래그십 AI 모델**이며, 주로 Google 제품 및 서비스(Bard/Gemini 챗봇, Google Cloud 등)를 통해 **폐쇄적으로 제공**됩니다.\n",
       "*   **Gemma (젬마)**는 Gemini의 연구를 기반으로 개발된 **경량의 오픈 소스 AI 모델 패밀리**로, 개발자와 연구자들이 **자유롭게 다운로드하여 사용하고 커스터마이징**할 수 있도록 설계되었습니다.\n",
       "\n",
       "다음은 더 자세한 비교입니다.\n",
       "\n",
       "---\n",
       "\n",
       "### Gemini (제미니)\n",
       "\n",
       "*   **목적 및 역할:** Google의 최첨단 기술을 대표하는 **플래그십 AI 모델**입니다. 복잡한 추론, 다중 모드(텍스트, 이미지, 오디오, 비디오) 이해 및 생성, 코딩, 창의적 작업 등 광범위한 고성능 작업을 수행하는 데 중점을 둡니다.\n",
       "*   **규모 및 성능:**\n",
       "    *   **매우 크고 강력합니다.** (Ultra, Pro, Nano 등 다양한 크기가 있지만, 전반적으로 Gemma보다 훨씬 큽니다.)\n",
       "    *   **다중 모드(Multimodal):** 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 생성할 수 있습니다.\n",
       "    *   **최고 수준의 성능:** 현존하는 AI 모델 중 가장 뛰어난 성능을 자랑하는 모델 중 하나입니다.\n",
       "*   **접근성:**\n",
       "    *   **폐쇄형/API 기반:** 직접 모델 파일을 다운로드하여 실행할 수 없습니다. Google의 서비스(예: Bard/Gemini 챗봇), Google Cloud Vertex AI, Google AI Studio 등의 API를 통해 접근하고 사용합니다.\n",
       "    *   **Google 제품 통합:** Google 검색, Workspace (Duet AI), Android 등 Google의 다양한 제품과 서비스에 통합되어 사용자 경험을 향상시킵니다.\n",
       "*   **대상:** 일반 사용자, 엔터프라이즈 고객, 복잡한 AI 솔루션을 구축하려는 개발자.\n",
       "\n",
       "---\n",
       "\n",
       "### Gemma (젬마)\n",
       "\n",
       "*   **목적 및 역할:** Gemini 개발에 사용된 연구와 기술을 기반으로 만들어진 **오픈 소스 모델 패밀리**입니다. 개발자와 연구자들이 AI 혁신을 가속화하고, 자신만의 AI 애플리케이션을 구축하며, 모델을 커스터마이징하고 연구할 수 있도록 지원하는 것이 주된 목적입니다.\n",
       "*   **규모 및 성능:**\n",
       "    *   **경량(Lightweight):** Gemini에 비해 훨씬 작고 효율적입니다. (예: 2B, 7B 파라미터 모델, 이후 9B, 27B 등 다양한 크기 출시)\n",
       "    *   **텍스트 기반:** 주로 텍스트 생성 및 이해에 특화되어 있습니다 (초기 버전 기준).\n",
       "    *   **효율성:** 제한된 컴퓨팅 자원에서도 실행될 수 있도록 최적화되어 있습니다.\n",
       "*   **접근성:**\n",
       "    *   **오픈 소스:** 모델 가중치와 코드를 공개하여 누구나 다운로드하여 사용하고 수정하며 재배포할 수 있습니다 (특정 사용 약관 하에).\n",
       "    *   **로컬 실행 및 미세 조정(Fine-tuning):** 개발자들이 자신의 서버, 심지어 일부 개인 장치에서도 모델을 실행하고 특정 작업에 맞게 미세 조정할 수 있습니다.\n",
       "*   **대상:** AI 개발자, 연구자, 학생, 스타트업, 제한된 자원으로 AI 솔루션을 구축하려는 사람들.\n",
       "\n",
       "---\n",
       "\n",
       "### 주요 차이점 요약\n",
       "\n",
       "| 특징           | Gemini (제미니)                                   | Gemma (젬마)                                      |\n",
       "| :------------- | :------------------------------------------------ | :------------------------------------------------ |\n",
       "| **성격**       | Google의 플래그십, 최첨단 AI 모델 (폐쇄형)       | Google의 오픈 소스 경량 AI 모델 패밀리          |\n",
       "| **접근 방식**  | API, Google 제품/서비스를 통해 접근               | 모델 파일 다운로드, 로컬 실행, 커스터마이징 가능 |\n",
       "| **규모/성능**  | 매우 크고 강력함, 다중 모드, 최고 수준의 성능     | 작고 효율적, 주로 텍스트 기반, 효율성 중시       |\n",
       "| **주요 용도**  | 복잡한 AI 작업, Google 제품 경험 향상             | 맞춤형 AI 앱 개발, 연구, 로컬 배포, 미세 조정    |\n",
       "| **타겟 사용자** | 일반 사용자, 엔터프라이즈, 고성능 AI 솔루션 개발자 | AI 개발자, 연구자, 학생, 소규모 프로젝트         |\n",
       "| **관계**       | 핵심 기술, 기반                                   | Gemini 연구에서 파생된 오픈 소스 버전             |\n",
       "\n",
       "---\n",
       "\n",
       "**간단한 비유:**\n",
       "\n",
       "*   **Gemini**는 **Google이 직접 운영하는 최고급 레스토랑의 시그니처 요리**와 같습니다. 당신은 그 요리를 맛볼 수는 있지만, 레시피나 주방 장비를 직접 사용할 수는 없습니다.\n",
       "*   **Gemma**는 **그 최고급 레스토랑의 셰프가 핵심 레시피를 바탕으로 집에서 쉽게 만들 수 있도록 공개한 간소화된 버전의 레시피 키트**와 같습니다. 당신은 그 키트를 사서 직접 요리하고, 원하는 대로 재료를 추가하거나 바꿔볼 수 있습니다.\n",
       "\n",
       "결론적으로, Gemini는 Google의 AI 역량을 보여주는 가장 강력한 결과물이며, Gemma는 그 역량을 외부 개발자들이 활용하고 확장할 수 있도록 문을 연 모델입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
