{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f840dcf4",
   "metadata": {},
   "source": [
    "# 2025.06.12. (THU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0a099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa0f26",
   "metadata": {},
   "source": [
    "## RAG(Retrieval Augmented Generation)\n",
    "\n",
    "* Search-based Generation(resolving Knowledge Cutoff, Hallucitnation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecb498",
   "metadata": {},
   "source": [
    "### vs Fine-Tuning\n",
    "\n",
    "* Fine Tuning\n",
    "    \n",
    "    * Pre-trained Model + Data of a certain domain\n",
    "\n",
    "    * High performance at a certain domain\n",
    "\n",
    "    * Cost and Time Issue. No Real-time.\n",
    "\n",
    "* RAG\n",
    "\n",
    "    * Real-time Search on External Information\n",
    "\n",
    "    * No need to modify the model.\n",
    "\n",
    "    * Performance varies in accordance with quality of documents\n",
    "\n",
    "    * Need to build a Search System\n",
    "\n",
    "    * **THE WAY IT SEARCH** really matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1feeb",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "* Indexing\n",
    "\n",
    "* Search/Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ed0c3",
   "metadata": {},
   "source": [
    "### 1. Indexing\n",
    "\n",
    "**Preprocessing**, **Saving in Vector DB**, **Getting ready to search**\n",
    "\n",
    "1. Load\n",
    "\n",
    "2. Split/Chunking\n",
    "\n",
    "3. Embedding\n",
    "\n",
    "4. Store\n",
    "\n",
    "    * RDB Obsatcles: It only search the identical data. Now we need SIMILARITY-based search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1295db5",
   "metadata": {},
   "source": [
    "### 2. Retrieval & Generation\n",
    "\n",
    "1. Retrieve\n",
    "\n",
    "2. Query\n",
    "\n",
    "3. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fadae",
   "metadata": {},
   "source": [
    "### Document loader\n",
    "\n",
    "* various langchain class for different type of resources\n",
    "\n",
    "    * e.g. Local(`.csv`, `.json`, etc.), DB Datasets, Internet data(API, Webpage Data, Files in cloud storages)\n",
    "\n",
    "* returns: `list(document)`\n",
    "    \n",
    "    * document attributes: `page_content`, `metadata`(option), `id`(option)\n",
    "\n",
    "* IT DOES NOT MATTER THAT WHEHER TO USE LANGCHAIN DOCUMENTLOADER OR OTHER LIBRARIES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be65a7",
   "metadata": {},
   "source": [
    "### `TextLoader` for text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "path = 'data/olympic.txt'\n",
    "\n",
    "# with open(path, 'r') as f:\n",
    "    # doc = f.read()\n",
    "\n",
    "loader = TextLoader(path ,encoding = 'utf-8')\n",
    "docs = loader.load()    # cf. .lazy_load() memory optimization\n",
    "print(type(docs), len(docs))\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa34fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': 'data/olympic.txt'}\n",
      "ID: None\n",
      "Content:\n",
      "올림픽\n",
      "올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\n",
      "또한 20세기에 올림픽 운동이 발전함에 따라, IOC는 변화하는 세계의 사회 환경에 적응해야 했다. 이러한 변화의 예로는 얼음과 눈을 이용한 경기 종목을 다루는 동계 올림픽, 장애인이 참여하는 패럴림픽, 스페셜 올림픽, 데플림픽, 10대 선수들이 참여하는 유스 올림픽 등을 들 수 있다. 그 뿐만 아니라 IOC는 20세기의 변화하는 경제, 정치, 기술 환경에도 적응해야 했다. 그리하여 올림픽은 피에르 드 쿠베르탱이 기대했던 순수한 아마추어 정신에서 벗어나서, 프로 선수도 참가할 수 있게 되었다. 올림픽은 점차 대중 매체의 중요성이 커짐에 따라 올림픽의 상업화와 기업 후원을 놓고도 논란이 생겨났다. 또한 올림픽을 치르며 발생한 보이콧, 도핑, 심판 매수, 테러와 같은 수많은 일들은 올림픽이 더욱 굳건히 성장할 수 있는 원동력이 되었다.\n",
      "올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.\n",
      "올림픽은 거의 모든 국가가 참여할 정도로 규모가 커졌다. 하계 올림픽은 33개의 종목과 약 400개의 세부종목에서 13,000명이 넘는 선수들이 겨루고 그중 각 종목별 1, 2, 3위는 각각 금/은/동을 수여받는다. 전 세계 언론에서 각각 4년마다 열리는 올림픽 경기를 중계하기 때문에 이름 없는 선수가 개인적, 국가적, 세계적으로 명성을 얻을 수 있는 기회가 된다. 이와 더불어 올림픽 경기는 개최지와 개최국에게도 전 세계에 그 이름을 널리 알리는 좋은 기회가 된다.\n",
      "\n",
      "고대올림픽\n",
      "고대의 올림픽 경기(올림피아 경기)는 고대 그리스의 여러 도시 국가의 대표선수들이 모여 벌인 일련의 시합이었으며, 육상 경기가 주 종목이지만 격투기와 전차 경기도 열렸다. 그리고 패배하면 죽기도 하였다. 고대 올림픽의 유래는 수수께끼로 남아있다. 잘 알려진 신화로는 헤라클레스와 그의 아버지인 제우스가 올림픽의 창시자였다는 것이다. 전설에 따르면 이 경기를 최초로 '올림픽'이라고 부르고, 4년마다 대회를 개최하는 관례를 만든 사람이 헤라클레스라고 한다. 어떤 전설에서는 헤라클레스가 이른바 헤라클레스의 12업을 달성한 뒤에 제우스를 기리고자 올림픽 경기장을 지었다고 한다. 경기장이 완성되자 헤라클레스는 일직선으로 200 걸음을 걸었으며, 이 거리를 \"스타디온\"이라 불렀는데, 후에 이것이 길이 단위인 '스타디온'(그리스어: στάδιον → 라틴어: 영어: stadium)이 되었다. 또 다른 설로는 '올림픽 휴전'(그리스어: ἐκεχειρία 에케케이리아[*])이라는 고대 그리스의 관념이 최초의 올림피아 경기와 관련이 있다고 한다. '올림픽 휴전'이란 어느 도시 국가라도 올림피아 경기 기간 중에 다른 나라를 침범하면 그에 대한 응징을 받을 수 있다는 뜻으로, \"올림픽 기간에는 전쟁하지 말 것\"으로 요약할 수 있다.\n",
      "고대 올림피아 경기가 처음 열린 시점은 보통 기원전 776년으로 인정되고 있는데, 이 연대는 그리스 올림피아에서 발견된 비문에 근거를 둔 것이다. 이 비문의 내용은 달리기 경주 승자 목록이며 기원전 776년부터 4년 이후 올림피아 경기 마다의 기록이 남겨져 있다. 고대 올림픽의 종목으로는 육상, 5종 경기(원반던지기, 창던지기, 달리기, 레슬링, 멀리뛰기), 복싱, 레슬링, 승마 경기가 있었다. 전설에 따르면 엘리스의 코로이보스가 최초로 올림피아 경기에서 우승한 사람이라고 한다.\n",
      "고대 올림피아 경기는 근본적으로 종교적인 중요성을 띄고 있었는데, 스포츠 경기를 할 때는 제우스(올림피아의 제우스 신전에는 페이디아스가 만든 제우스 상이 있음)와 펠롭스를 기리기 위하여 제물 봉헌 의식을 치렀다. 펠롭스는 올림피아의 전설상의 임금이었던 피사티스의 오이노마오스 왕과 전차 경주를 겨룬 영웅으로 유명한 인물이다. 올림피아 경기의 승자는 시와 조각상으로 칭송받았다. 올림피아 경기는 4년마다 열렸으며, 이 기간을 '올림피아드'(Olympiad)라고 했는데, 그리스인들은 이를 시간 단위로 이용하였다. 올림피아 경기는 고대 그리스에서 정기적으로 열렸던 범그리스 대회의 순환 대회 가운데 하나였다.\n",
      "올림피아 경기는 기원전 6세기~기원전 5세기에 절정에 이르렀으나, 그 후 로마가 패권을 잡은 뒤 그리스에 영향력을 행사하면서 서서히 쇠퇴하게 된다. 고대 올림픽이 공식적으로 끝난 해는 확실히 알 수 없으나, 대부분 테오도시우스 1세 황제가 모든 이단 숭배 및 예배를 금지했던 393년을 고대 올림픽의 마지막이라고 추정한다. 다른 설에 따르면 테오도시우스의 후계자인 테오도시우스 2세가 모든 그리스 신전을 파괴하라고 명령한 426년이라고도 한다. 이렇게 올림픽이 사라진 이후로 이보다 한참 뒤인 19세기에 이르러서야 비로소 다시 올림픽 경기가 열리게 된다.\n",
      "\n",
      "\n",
      "근대올림픽\n",
      "고대 올림피아 경기를 제대로 구현한 최초의 시도는 혁명 시대의 프랑스에서 1796년부터 1798년까지 3년동안 실시했던 프랑스 국내 올림픽인 '공화국 올림픽'(L'Olympiade de la République)이었다. 이 대회의 종목 중에는 고대 그리스 올림피아 경기 때 행한 일부 종목도 있었다. 특히 1798년 공화국 올림픽 대회는 미터법을 최초로 스포츠에 도입시킨 대회이기도 하다. 이후 52년뒤인 1850년에는 잉글랜드 슈롭셔주의 웬록에서 올림픽급의 대회가 열리기 시작하였다. 이 대회는 1859년에 아테네에서 열렸을 때 웬록 올림픽으로 명칭이 변경되었으며 지금도 열리고 있다. 브룩스 박사는 1859년에 아테네에서 열린 올림픽 경기의 내용을 이후 경기에 채택하였다. 1866년 런던의 수정궁에서는 윌리엄 페니 브룩스가 영국의 국가 올림픽 대회를 만들었다.\n",
      "1821년 그리스에서는 오스만 제국의 지배에 반기를 들고 독립 전쟁이 일어나면서, 이때부터 올림픽 부활에 대한 관심이 생겨났다. 시인이자 신문 편집자였던 파나요티스 수초스(Παναγιώτης Σούτσος)는 1833년에 출간한 자신의 시 '망자(亡者)의 대화'에서 최초로 올림픽 부활에 대한 제안을 내놓았다. 그리스의 부유한 박애주의자였던 에방겔리스 자파스(Ευαγγέλης Ζάππας)는 1859년에 아테네 시 광장에서 열린 \"올림픽 경기(일명 자파스 올림픽)\"를 후원하였다. 이 경기에는 그리스와 오스만 제국 출신의 선수들이 참가하였다. 에방겔리스 자파스는 이후에도 올림픽 경기를 개최할 수 있도록 고대의 경기장이었던 파나티네코 경기장을 복원하는 데도 돈을 썼다. 파나티네코 경기장에서 1870년과 1875년에 자파스 올림픽을 개최했으며, 현대 올림픽인 2004년 하계 올림픽 때는 양궁 경기장으로도 쓰였다.\n",
      "역사학자였던 쿠베르탱은 프로이센-프랑스 전쟁(1870–1871)에서 프랑스의 패배 원인을 분석하면서 군사들이 체계적인 체력 훈련을 받지 않았기 때문에 전쟁에서 패배했다고 말한 인물이다. 1890년 웬록 올림픽에 참석한 쿠베르탱은 그 이후부터 올림픽을 대규모로 부활시킬 수 있으리라 생각했다. 쿠베르탱은 웬록 올림픽과 자파스 올림픽을 토대로 하여 올림픽 경기를 국제적으로 시행하기 위해 나라별로 올림픽을 번갈아가며 개최하는 방식을 생각해냈다. 그는 이 방안을 새로 설립된 국제 올림픽 위원회(IOC)의 첫 올림픽 의회 기간 중에 언급했다. 총회는 파리의 소르본 대학교에서 1894년 6월 16일부터 6월 23일까지 7일간 지속되었으며, 총회 마지막날, 2년 후인 1896년에 아테네에서 국제적 규모의 올림픽 대회를 열기로 결정되었다. IOC는 올림픽을 조직하는 데에 모든 책임을 졌으며, 초대 위원장으로는 그리스의 작가였던 디미트리오스 비켈라스(Δημήτριος Βικέλας)가 선출되었다.\n",
      "\n",
      "하계올림픽\n",
      "1859년 자파스 올림픽에 참가한 선수의 수는 250명을 넘지 못했다. 에방겔리스 자파스는 \"지난 자파스 올림픽을 포함, 1896년에 개최될 2번째 올림픽을 위해 파나티네코 경기장을 보수해야 한다.\"라는 충고를 하지만, 그리스 정부는 그의 말을 듣지 않았고 결국 1896년 아테네 올림픽 준비를 위해 파나티네코 경기장은 두 번이나 정비해야 했다. 1회 대회 정식종목으로는 9종목이 있었는데 육상, 사이클, 펜싱, 체조, 사격, 수영, 테니스, 역도, 레슬링이 있었으며, 조정도 정식종목이었으나 매우 나쁜 날씨로 인해 조정 경기는 취소되었다. 펜싱 경기는 역사적 건물인 자피온(에반젤리스 자파스의 이름을 딴 것이다)에서 열렸다. 그리스의 관리들과 국민들은 올림픽 경기 개최에 열광적이었다. 많은 선수들이 이에 동감하면서 앞으로도 올림픽 대회를 아테네에서 영구히 개최해야 한다고 요구하기까지 하였다. 그러나 국제올림픽위원회(IOC)는 근대 올림픽은 순환 개최로 열리는 세계적인 행사가 되어야 한다고 생각했다. 결국 2회 올림픽은 프랑스 파리에서 열기로 결정되었다.\n",
      "1896년 올림픽 대회의 성공을 이어서 개최된 두 번째 올림픽인 1900년 올림픽에서는 올림픽의 존폐여부를 위협받는 지경에 이르게 되었다. 1900년에 파리와 1904년에 세인트루이스에서 열린 올림픽은 하필이면 엑스포와 시간과 장소가 겹치는 바람에 빛을 바래게 된다. 1904년 대회를 예로 들면 650명의 선수단이 참가했지만 그중 580명은 미국국적을 가진 사람이었다. 1900년과 1904년의 두 올림픽 대회는 역대 올림픽중에 최저점을 기록한다. 올림픽은 1906년 올림픽이 아테네에서 개최되었을 때 다시 일어서게 된다. 또 다른 성공적인 올림픽은 그리스 올림픽 협회가 조직했으며 세 차례나 올림픽을 치른 경기장에서 개최되었다. 이 경기는 비공식 올림픽이긴 했지만 세계적으로 상당한 참가자들을 불러 모았으며 대중들에게 큰 재미를 갖다주었다. 이 때를 시작으로 올림픽의 인기와 번영이 시작되었다.\n",
      "\n",
      "동계올림픽\n",
      "동계 올림픽은 눈과 얼음을 이용하는 스포츠들을 모아 이루어졌으며 하계 올림픽 때 실행하기 불가능한 종목들로 구성되어 있다. 피겨스케이팅, 아이스하키는 각각 1908년과 1920년에 하계올림픽 종목으로 들어가 있었다. IOC는 다른 동계 스포츠로 구성된 새로운 대회를 만들고 싶어 했고, 로잔에서 열린 1921년 올림픽 의회에서 겨울판 올림픽을 열기로 합의했다. 1회 동계올림픽은 1924년, 프랑스의 샤모니에서 11일간 진행되었고, 16개 종목의 경기가 치러졌다. IOC는 동계 올림픽이 4년 주기로 하계 올림픽과 같은 년도에 열리도록 했다. 이 전통은 프랑스의 알베르빌에서 열린 1992년 올림픽 때까지 지속되었으나, 노르웨이의 릴레함메르에서 열린 1994년 올림픽부터 동계 올림픽은 하계 올림픽이 끝난지 2년후에 개최하였다.\n",
      "\n",
      "패럴림픽\n",
      "패럴림픽(Paralympic)은 신체·감각 장애가 있는운동 선수가 참가하는 국제 스포츠 대회로, 장애인 올림픽으로 불린다. 1948년에 루드비히 구트만 경(Sir Ludwig Guttman)은 제2차 세계대전에 참전한 군인들의 사회 복귀를 위한 일환으로 1948년 런던 올림픽과 동시에 몇몇 병원들을 연합해서 여러 경기를 펼쳤다. 구트만의 세계 휠체어, 신체부자유자대회(World Wheelchair and Amputee Games)로 알려진 이 대회는 매년 열리는 스포츠대회가 되었다. 12년이 넘도록 구트만과 다른 사람들은 스포츠를 상처를 치료하는 방법 중 하나로써 계속 대회 개최에 노력을 기울였다. 로마에서 열린 1960년 하계 올림픽때 구트만은 400명의 선수들을 \"Parallel Olympics\"에 참가시켰으며 이것이 곧 1회 패럴림픽으로 알려지게 되었다. 그 때부터 패럴림픽은 하계 올림픽이 열린 년도에 열리게 되었다. 서울에서 열린 1988년 하계 올림픽부터는 하계 올림픽을 개최한 도시는 패럴림픽도 같이 개최하기로 한다.\n",
      "\n",
      "오늘날의 올림픽\n",
      "1896년 대회때는 14개국에서 241명의 선수단이 참가했지만 2008년 하계 올림픽때는 204개국에서 10,500명의 선수가 참가하는 등 세계적인 대회로 변모했다. 동계 올림픽의 규모는 하계 올림픽 규모보다 작다. 예를 들면 2006 토리노 동계 대회때는 80개국에서 2,508명의 선수가 참가했으며 82개 세부종목이 있었고, 2008 베이징 하계 대회때는 204개국, 11,508명의 선수, 302개의 세부종목이 있었다. 올림픽이 진행되는 동안 선수와 임직원들은 올림픽 선수촌에서 지낸다. 올림픽 선수촌에는 선수들을 위한 개인실이 있으며 카페테리아, 헬스 클리닉, 종교적인 시설 등 최상의 편의를 위한 시설들이 있다.\n",
      "올림픽에 참가하는 나라는 UN에 등록된 국가의 수 193개보다 많다. 다른 국제조직이 개최하는 대회들은 정치적 주권국으로 참가를 제한하는 반면, IOC는 그에 상관없이 올림픽에 모든 공동체들이 참가할 수 있도록 한다. 이는 연합체나 공동체에서 국가올림픽위원회(NOC)를 만드는 것을 허용한다는 의미이다. 예를 들면 푸에르토리코, 버뮤다, 홍콩과 같은 곳도 올림픽에서 다른 나라와 스포츠 경쟁을 합법적으로 할 수 있다.\n",
      "\n",
      "국제 올림픽 위원회\n",
      "올림픽 활동이란 많은 수의 국가, 국제 경기 연맹과 협회 • 미디어 파트너를 맺기 • 선수, 직원, 심판, 모든 사람과 기관이 올림픽 헌장을 지키는 것을 말한다. 국제올림픽위원회(IOC)는 모든 올림픽 활동을 통솔하는 단체로서, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 권리가 있다. 올림픽 활동은 크게 세 가지로 구성된다.\n",
      "- 국제경기연맹(IF)은 국제적인 규모의 경기를 관리, 감독하는 기구이다. 예를 들어서 국제 축구 연맹(FIFA)는 축구를 주관하며, 국제 배구 연맹(FIVB)은 배구를 주관하는 기구이다. 올림픽에는 현재 35개의 국제경기연맹이 있고 각 종목을 대표한다. (이 중에는 올림픽 종목은 아니지만 IOC의 승인을 받은 연맹도 있다.)\n",
      "- 국가 올림픽 위원회(NOC)는 각국의 올림픽 활동을 감독하는 기구이다. 예를 들어서 대한 올림픽 위원회(KOC)는 대한민국의 국가 올림픽 위원회이다. 현재 IOC에 소속된 국가 올림픽 위원회는 205개이다.\n",
      "- 올림픽 조직 위원회(OCOG)는 임시적인 조직으로 올림픽의 총체적인 것(개막식, 페막식 등)을 책임지기 위해 구성된 조직이다. 올림픽 조직 위원회는 올림픽이 끝나면 해산되며 최종보고서를 IOC에 제출한다.\n",
      "올림픽의 공식언어는 프랑스어와 영어와 개최국의 공용어이다. 모든 선언(예를 들어서 개막식 때 각국 소개를 할 때)들은 세 언어가 모두 나오거나 영어나 프랑스어 중에서 한 언어로만 말하기도 한다. 개최국의 공용어가 영어나 프랑스어가 아닐 때는 당연히 그 나라의 공용어도 함께 나온다.\n",
      "\n",
      "국제 올림픽 위원회(이하 IOC로 지칭)는 몇몇 위원들이 한 행위에 대해서 비판을 받고 있다. 그 예로 IOC 위원장이었던 에이버리 브런디지와 후안 안토니오 사마란치가 대표적인 사람이다. 브런디지는 20년 넘게 IOC 위원장직을 맡았고 임기 중에 올림픽을 정치적으로 휘말려들지 않게 하기 위해 보호했다. 그러나 그는 남아프리카 공화국 대표단에게 아파르트헤이트와 관련된 이슈를 건드리고 반유대정책을 함으로써 비난을 받았다. 사마란치 위원장 시기 때는 족벌 정치와 부패로 비난받았다. 사마란치가 스페인에서 프랑코 정권에 협력했다는 것도 비판의 이유가 되었다.\n",
      "1998년에 몇몇 IOC위원들이 2002년 솔트레이크 시티 동계 올림픽 유치 과정에서 미국에게 미국을 올림픽 개최지로 뽑아달라는 뇌물청탁을 받았다는 것이 폭로되었다. 이에 IOC는 사퇴한 IOC위원 4명과 강제 퇴출된 6명에 대한 조사를 했다. 이 스캔들은 이후에 개최지 선정에서 이와 같은 불미스러운 일이 일어나지 않게 하기 위해서 IOC가 개혁에 착수하도록 하는 긍정적인 역할을 하기도 했다.\n",
      "BBC 다큐멘터리인 '파노라마'에서는 '매수된 올림픽'이란 주제로 2004년 8월에 방송을 내보내기도 했다. 이때 이 프로그램에서는 2012년 하계 올림픽의 개최지 선정과 관련된 뇌물에 대해서 조사했다. 이 다큐멘터리에서는 특정 후보 도시가 IOC 위원들에게 뇌물수수하는 것이 가능했다고 주장했으며, 특히 파리 시장이었던 베르트랑 들라노에(Bertrand Delanoë)는 영국의 총리인 토니 블레어와 런던올림픽유치위원회가 입후보 규정을 위반했다고 비난했다. 그는 당시 프랑스 대통령이었던 자크 시라크를 목격자로 내세웠지만 시라크 대통령은 이 분쟁에 휘말려드는 것을 주의했으며 인터뷰를 삼갔다. 결국 베르트랑 들라노에의 주장에 대한 조사는 체계적으로 이루어지지는 않았다. 2006년 동계 올림픽을 유치했던 토리노도 이 논쟁에서 빠져나갈 수 없었다. 이번에는 스위스 국적의 IOC위원 마크 호들러(Marc Hodler)가 이 논쟁의 중심이 되었는데, 이 위원은 스위스 시온의 경쟁 도시였던 토리노가 IOC위원들에게 뇌물수수를 했다고 말했고, 이 발언으로 광범위한 조사가 이루어졌다. 이 언행이 많은 IOC위원들이 시온에 대해 언짢게 생각하게 되고 토리노가 개최지로 선정되도록 도와주는 역할을 했을 가능성도 제기되고 있다.\n",
      "\n",
      "올림픽 경기 종목\n",
      "올림픽 경기 종목은 총 33개부문 52개 종목에서 약 400개의 경기로 이루어져있다. 예를 들어서 하계 올림픽 부문인 레슬링은 자유형과 그레코로만형의 두 종목으로 나뉜다. 여기에서 10경기는 남자부, 4경기는 여자부로 열리며 분류기준은 체중이다. 하계 올림픽은 26개, 동계 올림픽은 7개 부문으로 이루어져있다. 하계 올림픽에서는 육상, 수영, 펜싱, 체조가 1회 대회때부터 한번도 빠짐없이 정식종목이었으며, 동계 올림픽에서는 크로스컨트리, 피겨 스케이팅, 아이스 하키, 노르딕 복합, 스키 점프, 스피드 스케이팅이 1924년 동계 올림픽부터 빠짐없이 정식종목이었다. 배드민턴, 농구, 배구와 같은 정식종목들은 처음에는 시범종목이었으며 그 후에 정식종목으로 승인 되었다. 야구처럼 예전에는 정식종목 이었지만 지금은 정식 종목에서 빠진 종목도 있다.\n",
      "\n",
      "각 올림픽 종목들은 IOC로부터 승인을 받은 국제경기연맹의 관리를 받는다. 35개의 연맹이 IOC에서 승인을 받았으며, 승인을 받았지만 현재 정식종목이 아닌 종목을 감독하는 연맹도 있다. IOC의 승인을 받았지만 올림픽 종목이 아닌 스포츠들은 올림픽 종목으로 고려되지는 않으나, 올림픽이 끝난 후 처음으로 열리는 IOC총회 때마다 정식종목이 되도록 신청을 할 수는 있다. IOC 총회 때 정식종목 선정은 총회에 참석중인 IOC위원들의 투표를 통해 이루어지며, 재적 위원 수의 과반수 이상 찬성표를 얻어야 정식종목으로 인정을 받는다. IOC의 승인을 받은 스포츠이나 찬성표를 받지 못해 정식종목이 되지 못한 스포츠로는 체스와 서핑과 같은 것이 있다.\n",
      "\n",
      "2004년 10월과 11월에 IOC는 '올림픽 프로그램 위원회'(Olympic Programme Commission)를 설립했다. 여기서는 올림픽 종목과 올림픽 종목이 아닌 스포츠를 모두 재검토하는 일을 한다. 이 위원회의 목표는 올림픽 종목에 더 체계적으로 다가가는 것이다. 위원회에서는 우선적으로 올림픽 종목으로 포함되기 위해서는 7개의 기준을 충족시켜야 한다고 말한다. 이 7개의 기준은 역사, 전통, 보편성, 인기도와 잠재성, 선수의 건강, 연맹의 스포츠를 관리할만한 능력, 스포츠를 여는 데에 필요한 비용이다. 예를 들면 2012년 하계 올림픽의 정식종목 후보에 7개 조건을 포함한 비(非)올림픽 스포츠가 올랐고 그 내용은, 골프, 가라테, 럭비, 인라인 스케이팅, 스쿼시였다. 이 스포츠들은 IOC 상임이사회에서 재검토되어 2005년 7월에 열린 싱가포르 총회에서 최종 결정하기로 했다. 결국 5개 중 2개(가라테와 스쿼시) 가 최종 후보로 올라왔으나 가라테와 스쿼시 둘 다 2/3의 미만의 찬성표로 정식종목이 되지는 못한다. 그 후 2016년 올림픽 정식종목에는 7개의 스포츠가 정식종목 신청을 했는데, 내용은 가라테, 골프, 스쿼시, 야구, 소프트볼, 7인제 럭비, 인라인 스케이팅이었다. 2009년 8월 13일, 신청된 7개의 스포츠 중 단 2개만 최종후보로 선정되었는데, 이는 7인제 럭비와 골프였다. 같은해인 2009년 10월에 열린 IOC 총회에서 골프와 럭비는 과반수 이상의 득표를 얻어서 2016년 하계 올림픽과 2020년 하계 올림픽의 정식종목으로 채택되었다.\n",
      "2002년에 열린 제114차 IOC 총회에서는 하계 올림픽 종목은 최대 28부문 301개 경기에 10,500명이 참가하는 것으로 제한하기로 결정했다.그 후 3년 뒤인 제117차 IOC 총회에서는 정식종목이었던 야구와 소프트볼을 정식 종목에서 제외시킨다. 이 결과에 대한 이견이 없었으므로 2012년 올림픽 때는 26개부문에서 경기가 열린다. 2016년과 2020년 올림픽 때는 럭비와 골프가 추가되어 다시 28개부문에서 경기가 열린다.\n",
      "프로 NHL선수들은 1998년부터 아이스 하키종목에 출전할 수 있게 되었다. (나가노 올림픽 결승전 러시아 vs 체코).\n",
      "영국 명문 공립 학교의 이념은 쿠베르탱에게 큰 영향을 끼쳤다. 영국 공립 학교는 스포츠를 교육의 중요한 부분이라 생각해서 '건전한 신체에 건전한 정신을'이라는 의미를 가진 라틴어 mens sana in corpore sano를 표어로 삼았다. 이 이념에 의하면 신사들은 특정한 분야에서만 우수해서는 안되고 모든 분야에서 고르게 잘해야 하고, 공정한 결과에는 승복해야 하며, 연습이나 훈련은 속이는 것과 마찬가지로 여겼다. 전문적으로 스포츠를 연습한 사람은 취미로 연습한 사람에 비해 공평하지 않다고 생각한 것이다.\n",
      "\n",
      "현대 올림픽에서는 프로 선수의 참가 불허가 많은 분쟁을 가져왔다. 1912년 하계 올림픽의 근대 5종 경기와 10종 경기에서 우승한 짐 소프는 올림픽에 나가기 전에 준프로야구선수로 활동했다는 게 나중에 밝혀져 메달이 박탈되었다. 소프는 후에 동정적 여론의 힘을 업고 1983년에 메달을 돌려받게 된다. 1936년 동계 올림픽 때 스위스와 오스트리아 스키선수들은 돈을 벌기 위해 스포츠를 했는데 이러한 행동이 아마추어 정신에 위배된다고 결정되어 그들은 스키종목에 참가할 수 없었다.\n",
      "20세기에 이르러서 계급구조가 붕괴되면서 이른바 귀족적인 신사라는 아마추어 선수에 대한 정의는 시대에 뒤처지는 말이 되게 된다. 일부 국가들은 '정식 아마추어 선수'를 '키워서' 순수한 아마추어 정신을 벗어나고 있었고, 자신이 내는 비용으로 연습하는 선수들의 불리함에 대한 목소리가 나오기 시작했다. 하지만 IOC는 아마추어 정신에 관한 입장을 고수했다. 1970년대 초에는 아마추어 정신이 올림픽헌장에서 폐지되어야 한다는 말이 나오기 시작했다. 결국 프로선수들의 출전은 국제경기연맹(IF)에서 결정짓도록 되었다. 2008년 기준으로 아마추어 선수만 출전하고 있는 올림픽 종목은 복싱이 유일하며 남자 축구에서는 나이가 23세 이상인 선수를 3명까지만 선발할 수 있다. 이는 아마추어 정신을 지키기 위한 일환으로 볼 수 있다.\n",
      "\n",
      "논란\n",
      "올림픽에서 첫 번째 보이콧은 1956년 하계 올림픽에서 시작되었다. 네덜란드, 스페인, 스위스는 소련의 헝가리 침공에 항의해 참가를 거부했다. 캄보디아, 이집트, 이라크, 레바논은 제2차 중동 전쟁 때문에 보이콧했다. 1972년과 1976년 올림픽에는 많은 아프리카의 국가들이 남아프리카 공화국과 로디지아에서 일어나는 인종 차별정권에 대한 항의의 표시로 올림픽 참가를 거부했다. 이 보이콧에는 뉴질랜드도 관계가 되어있는데, 뉴질랜드 럭비 국가 대표팀이 당시 아파르트헤이트정책을 쓰던 남아프리카 공화국과 경기를 했음에도 불구하고 뉴질랜드의 올림픽 참가가 허용되었기 때문이었다. 국제 올림픽 위원회는 이 두 보이콧에 대해 심각하게 고민했으나 후자의 뉴질랜드의 경우는 럭비가 올림픽 종목이 아니라는 이유를 내세워 뉴질랜드의 올림픽 참가 금지 요청을 거부했다. 당시 아프리카에 속해 있던 20개국과 가이아나, 이라크는 경기를 끝낸 선수들이 있었지만 탄자니아가 이끄는 올림픽 보이콧에 가세했다. 중화민국(타이완)도 1976년 몬트리올 올림픽 참가를 보이콧했는데, 그 이유는 중화인민공화국(중국)이 몬트리올 올림픽 조직위원회에게 타이완을 '중화민국'의 이름으로 참가하지 못하도록 압박을 가했기 때문이다. 타이완은 이것에 반발해서 중화민국의 국기와 중화민국의 국가를 계속 쓸 것이라고 밝혔다. 타이완은 1984년까지 올림픽에 참가하지 않았으며 그 후 참가할 때는 중화 타이베이 올림픽기와 특별한 찬가를 사용한다. 1980년과 1984년 올림픽 때는 냉전의 당사국들이 각각 반대진영에서 개최된 올림픽에 불참했다. 1980년에 열린 모스크바 올림픽 때는 소련의 아프가니스탄 침공에 대한 항의의 표시로 미국을 비롯한 65개국이 불참해서 1956년 이후 가장 적은 국가의 수인 81개국만 참가하는 대회가 되었다. 1984년에 열린 L.A 올림픽때는 루마니아와 유고슬라비아를 제외한 소련과 동구권의 14개 국가가 자국 선수들의 안전을 보장받지 못한다는 이유로 올림픽에 불참했다. 소련의 한 관계자는 그들이 올림픽 보이콧을 한 것에 대해 다음과 같은 발언을 통해 지지했다.\n",
      "\"미국에서 광적인 애국심과 반소련 세력이 점점 늘어나고 있다.\"\n",
      "동구권에서 보이콧을 한 국가들은 올림픽을 대신할 대회로 프렌드십 게임을 7월과 8월에 했다.\n",
      "2008년에는 티베트와 다르푸르에 관한 중국의 인권문제를 두고 그에 대한 항의 표시로 중국산 물품의 불매운동과 2008 올림픽 불참에 대한 요구가 컸으나 보이콧을 한 나라는 없었다. 2008년 8월, 조지아 정부는 러시아가 2008년 남오세티야 전쟁에 참전한 것과 관련하여 러시아의 소치에서 열릴 2014년 동계 올림픽을 보이콧하자고 요청했다. 이에 대해 국제 올림픽 위원회는 \"앞으로 개최될 때까지 6년이나 남았는데 시작하기도 전에 섣불리 이른 판단을 하는 것은 옳지 않다.\"라고 말했다.\n",
      "\n",
      "쿠베르탱이 말했던 원래 이념과는 반대로 올림픽이 정치 혹은 체제 선전의 장으로 이용되는 경우가 있었다. 1936년 하계 올림픽을 개최할 때 당시의 나치독일은 나치는 자비롭고 평화를 위한다는 것을 설명하고 싶어했다. 또 이 올림픽에서 아리안족의 우월함을 보여줄 생각이었으나 이는 흑인이었던 제시 오언스가 금메달을 4개나 따내면서 실현되지는 못했다. 소련은 헬싱키에서 열린 1952년 하계 올림픽 때 처음으로 참가했다. 그 전에는 소련이 조직한 스파르타키아다라는 대회에 1928년부터 참가했었다. 다른 공산주의 국가들은 1920년대와 1930년대의 전쟁 기간 사이에 노동자 올림픽(Socialist Workers' Sport International)을 조직했는데, 이는 올림픽을 자본가와 귀족들의 대회로 여기고 그에 대한 대안으로 고안된 대회였다. 그 이후 소련은 1956년 하계 올림픽부터 1988년 하계 올림픽까지 엄청난 스포츠강국의 면모를 보여주며 올림픽에서의 명성을 드높였다.\n",
      "선수 개인이 자신의 정치적 성향에 대해 표현하기도 했다. 멕시코 시티에서 열린 1968년 하계 올림픽의 육상부문 200m 경기에서 각각 1위와 3위를 한 미국의 토미 스미스와 존 카를로스는 시상식 때 블랙 파워 설루트(Black Power salute , 흑인 차별 반대 행위)를 선보였으며 2위를 한 피터 노먼도 상황을 깨닫고 스미스와 카를로스의 행위를 지지한다는 뜻에서 급하게 인권을 위한 올림픽 프로젝트(OPHR) 배지를 달았다. 이 사건에 대해서 IOC 위원장이었던 에이버리 브런디지는 미국 올림픽 위원회에 이 두 선수를 미국으로 돌려보내거나 미국 육상팀 전부를 돌려보내는 둘 중 하나의 선택을 하게 했고, 미국 올림픽 위원회는 두 선수를 미국으로 돌려 보낸다.\n",
      "현재 이란 정부는 이스라엘과의 어떤 경기 경쟁이든 피하고 있다. 2008년 하계 올림픽 때 이란의 수영 선수는 이스라엘 수영 선수와 같이 경기한다는 이유로 경기를 포기했으며, 2004년 하계 올림픽에서도 이란의 유도 선수는 이스라엘 선수와 경기한다는 일정이 잡혔을 때 경기를 포기했다. 이 선수는 공식적으로는 시합전에 계체량을 재서 체중이 초과되어 실격 되었으나 이란정부로부터 125,000달러나 되는 돈을 받았다고 한다.\n",
      "\n",
      "20세기 초반, 많은 운동 선수들은 기록향상을 위해 약물을 복용하기 시작했다. 예를 들어 1904년 하계 올림픽 마라톤에서 우승한 미국 선수 토머스 J. 힉스는 코치에게서 스트리크닌과 브랜디를 받았다. 올림픽에서 약물을 과다 복용으로 사망한 사례도 한 번 있었다. 1960년 로마 대회 때 사이클 개인도로 경기 중에 덴마크 선수인 크누드 에네마르크 옌센이 자전거에서 떨어져서 사망했다. 검시관들의 조사에 의하면 그의 죽음의 원인은 암페타민 과다 복용이라고 했다. 이에 1960년대 중반부터 각 경기 연맹은 약물 복용을 금지하기 시작했으며 1967년에는 IOC도 약물 복용 금지에 동참했다.\n",
      "올림픽에서 약물 복용 양성 반응이 나와서 메달을 박탈당한 첫 번째 사례로는 1968년 하계 올림픽의 근대 5종 경기에 출전해 동메달을 딴 한스 군나르 리렌바르가 있다. 그는 경기 후 도핑검사 결과 알코올을 복용한 것으로 확인되어 메달을 박탈당했다. 도핑 양성 반응으로 메달을 박탈당한 것으로 가장 유명한 사람은 1988년 하계 올림픽 육상 100m 경기에서 금메달을 땄으나 도핑 검사 결과 스타노졸롤을 복용한 것으로 확인돼 금메달을 박탈당한 캐나다 선수인 벤 존슨이 있다. 이에 따라 금메달은 2위를 했던 칼 루이스가 대신 받았다.\n",
      "1990년대 후반, 여러 뜻있는 사람들이 도핑과의 전쟁을 선포하면서 1999년에 세계반도핑기구(WADA)를 설립한다. 2000년 하계 올림픽과 2002년 동계 올림픽 때는 약물 양성 반응을 보인 선수들이 급격히 증가했고, 역도와 크로스컨트리에서는 몇몇 선수들이 도핑 테스트에 걸려서 실격되기도 했다. 2006년 동계 올림픽 때는 메달리스트 한 명이 양성반응을 보여 메달을 반납해야 했다. IOC가 만든 약물 반응 판정(현재 올림픽 도핑테스트의 기준이 됨)은 인정을 받게 되었고 이제는 다른 경기 연맹에서도 벤치마킹을 할 정도가 되었다. 2008년 베이징 올림픽 기간중에는 3,667명의 선수들이 세계반도핑기구의 검사를 받았으며 소변과 혈액 검사로 약물 복용 검사를 했다. 몇몇 선수들은 국가 올림픽 위원회(NOC)에 의해 올림픽이 시작되기 전에 출전금지 조치를 당했고, 올림픽 기간중에는 단 3명만이 도핑 검사에 걸렸다.\n",
      "쿠베르탱의 생각과는 달리, 올림픽이 세계에 완벽한 평화를 가져다주지는 못했다. 실제로 제1차 세계대전으로 인해 독일 베를린에서 열리기로 했던 제6회 1916년 하계 올림픽이 취소되었고, 제2차 세계대전 때는 일본 도쿄에서 열리기로 했던 제12회 1940년 하계 올림픽, 삿포로에서 열리기로 했던 1940년 동계 올림픽, 영국 런던에서 열리기로 했던 제13회 1944년 하계 올림픽, 이탈리아 코르티나담페초에서 열릴 예정인 1944년 동계 올림픽이 취소되었다. 베이징에서 열린 2008년 하계 올림픽 개막식날 조지아와 러시아 간의 2008년 남오세티아 전쟁이 일어나기도 했다. 부시 대통령과 푸틴 대통령이 이 올림픽을 보러 왔으며 중국 주석인 후진타오가 주최한 오찬에 참석해서 이 현안에 대해 논의하기도 했다. 조지아 대표인 니노 살루크바체와 러시아 대표인 나탈리야 파데리나가 여자 10m 공기권총 경기에서 각각 동메달과 은메달을 땄을 때 이 일은 베이징 올림픽의 유명한 사건 중 하나로 남게 되었다. 살루크바체와 파데리나는 시상식이 끝난 뒤 서로 포옹을 하며 국적에 상관없이 기쁨을 나누었다.\n",
      "테러도 올림픽에서 공포의 대상이었다. 뮌헨 참사로 알려진 1972년에 서독 바이에른의 뮌헨에서 열린 하계 올림픽때의 사건은 테러리스트인 검은 9월단이 일으킨 사건으로서 이스라엘 선수 11명을 인질로 붙잡았다가 전원이 사망한 사건이다. 당시 미숙한 진압으로 인해 인질 9명(선수 1명과 코치 1명은 인질로 잡기 이전에 살해), 테러범 5명, 독일 경찰관 1명이 사망했으며 이 진압 작전 이전에는 인질들은 단 한 명도 죽지 않았다. 애틀란타에서 열린 1996년 하계 올림픽 때는 센테니얼 올림픽 공원(Centennial Olympic Park)에서 폭발 사건이 일어나 2명이 죽고 111명이 다치는 사건이 발생했다. 이 사건의 주모자 에릭 로버트 루돌프는 종신형을 선고받았다. 참고로 마라톤 역시 전쟁에서 유래한 것이다.\n",
      "\n",
      "개최지 선정\n",
      "올림픽 개최지는 해당 올림픽 개최 7년 전에 IOC 위원들의 투표로 결정된다. 개최지 선정에는 약 2년이 걸린다. 유치를 희망하는 도시는 우선 자국의 올림픽 위원회에 신청을 해야 한다. 만약 한 국가에서 두 도시 이상이 유치를 희망한다면, 한 국가당 한 도시만 후보가 될 수 있다는 규칙에 따라 내부적으로 후보 도시를 결정해야 한다. 후보 도시가 결정되면 후보 도시가 소속된 국가의 올림픽 위원회는 IOC에 개최 신청을 하고, 신청 후에는 올림픽 개최에 대한 질의 응답서를 보내야 한다. 이 질의응답서에서 신청한 도시는 올림픽 헌장을 준수하며 IOC 상임이사회에 의한 다른 규정들을 지킬 것이라는 확신을 주어야 한다. 이 질의응답서는 전문가들이 검토하여 신청 도시들의 잠재성과 계획을 평가한다. 이 전문적인 평가를 바탕으로 IOC 상임이사회에서는 신청도시 중에서 후보도시를 고른다.\n",
      "후보도시로 선택되면 그 도시들은 IOC에 보내는 후보도시에 관한 문서에 그들의 계획을 더욱 상세하고 방대한 양으로 적어서 보내야 한다. 평가조사단들이 이 후보도시들을 평가한다. 평가조사단은 후보도시들을 방문해서 지역 관계자들과 회견을 갖고 경기장 시설을 세심하게 조사한 뒤 개최지 투표를 하기 한달전에 조사를 바탕으로 한 공식 보고를 한다. 회견을 하는 동안에도 후보도시들은 자신들이 올림픽을 개최하는 데 충분한 자금이 조달될 수 있는지 등을 입증할 수 있어야 한다. 평가조사단의 업무가 끝나면 후보지의 국가 위원들은 IOC 정기총회에 참석한다. 이 총회에서 IOC 위원들은 올림픽 개최지를 선정하게 되며 후보지의 국가에 소속된 위원들은 자국의 후보지가 탈락하지 않는 이상 투표를 할 수 없다. 투표가 끝난후에 개최지로 선정된 곳의 유치위원회가 IOC와 개최도시 계약서에 서명을 하면 공식적으로 올림픽 개최도시(개최국)으로 인정된다.\n",
      "2016년까지 올림픽은 23개국 44개 도시에서 열렸으며 유럽과 북아메리카대륙 이외의 대륙에서는 고작 8번 밖에 개최하지 못했다. 1988년 하계 올림픽이 대한민국의 서울에서 열린것을 시작으로 그 후 아시아와 오세아니아 대륙에서 올림픽이 4번이나 열렸으며, 이는 그 이전의 현대 올림픽사와 비교해보면 엄청나게 늘어난 수치였다. 2016년 하계 올림픽이 개최된 브라질의 리우데자네이루는 남미에서 열리는 첫 번째 올림픽이다. 아직 아프리카에서는 올림픽이 한 번도 개최되지 않았다. 2008년 하계 올림픽 때 가장 많은 선수가 참여한 나라는 중국으로 639명이 참가했으며 그 다음은 미국과 러시아로 각각 596명과 455명이 참가했다.\n",
      "미국은 5번의 하계 올림픽과 4번의 동계 올림픽을 개최하면서 최다 올림픽을 개최한 나라이다. 영국은 2012년에 3번째 올림픽을 개최하였다. 독일, 오스트레일리아, 그리스는 하계 올림픽을 2번 개최한 국가이다. 동계 올림픽에서는 이탈리아가 2026년 밀라노-코르티나담페초 개최지로 선정되어 3번 개최될 예정이다. 또한 프랑스가 3번을 개최했으며 2024년 하계올림픽 개최예정으로 영국에 이번 두 번째로 한 도시에서 3번 올림픽 개최하며 하계올림픽3번 개최하였다. 프랑스는 동계, 하계 올림픽 각 3번씩 총 6번 개최로 9번으로 최다개최국인 미국 다음으로 두 번째로 많이 개최한 국가가 된다. 스위스, 오스트리아, 노르웨이, 일본, 이탈리아는 2번씩 개최했다. 일본은 하계,동계 각 2번씩 총 4번으로 미국, 프랑스 다음 세번째로 많이 개최한 국가이다. 2010년에 밴쿠버에서 열린 2010년 동계 올림픽은 캐나다에서 열리는 두 번째 동계 올림픽이고, 동/하계 올림픽을 합쳐 캐나다에서 3번째로 개최되는 올림픽이다.\n",
      "\n",
      "우승자와 메달리스트\n",
      "개인 혹은 팀으로 경기에 출전해서 1위, 2위, 3위를 한 선수는 메달을 받는다. 1912년까지는 우승자에게 순금으로 된 금메달을 주었으며 그 후에는 도금된 금메달을 준다. 하지만, 2010 동계 올림픽에서는 전자제품 부속품을 녹여서 넣었다. 이러한 경우처럼 순금 외에 다른 물질을 넣을 경우에는 순금이 반드시 6g 이상을 함유하고 있어야 한다. 2위를 한 선수는 은메달을, 3위를 한 선수는 동메달을 받는다. 토너먼트로 진행되는 종목의 경우에는(복싱, 태권도 등) 3위를 구분하지 않고 준결승에서 패해서 3/4위전으로 간 선수들에게 모두 동메달을 수여한다. 1896년 하계 올림픽에서는 메달이 2개만 수여됐는데 1위에게 은메달을 주었고 2위에게 동메달을 주었다. 이때 3위에게는 아무것도 없었다. 현재의 메달 수여 방식은 1904년 하계 올림픽 때부터 시작되었다. 1948년부터는 4, 5, 6위를 한 선수에게는 인증서를 수여했다. 1984년 대회부터는 7, 8위를 한 선수에게도 인증서를 수여했다. 아테네에서 열린 2004년 하계 올림픽 때는 1, 2, 3위 선수에게 메달과 함께 올리브 화환도 같이 수여했다. 국가 올림픽 위원회(NOC)와 방송사에서는 자국의 메달 현황을 실시간으로 전달하기도 한다.\n"
     ]
    }
   ],
   "source": [
    "doc = docs[0]\n",
    "print(\"Metadata:\", doc.metadata)\n",
    "print(\"ID:\", doc.id)\n",
    "print('Content:')\n",
    "print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0187101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentLoader Do the following tasks:\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    load_doc = f.read()\n",
    "\n",
    "d = Document(page_content = load_doc, metadata = {'category':'Olympic', 'path':path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310d9cd",
   "metadata": {},
   "source": [
    "### PDF\n",
    "\n",
    "* PyPDF, Pymupdf, etc.\n",
    "\n",
    "* `PyMuPDFLoader`: High performance not only at text, but also at images, annotation, etc.\n",
    "\n",
    "* `PyPDFLoader`: Especially good at text.\n",
    "\n",
    "* `PDFPlumberLoader`: Table, text, image, etc. Especially good at complex data structure(e.g. table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path = 'data/novel/금_따는_콩밭_김유정.pdf'\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs) # total pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b2b8599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "위키백과\n",
      "위키백과에  이  글\n",
      "과  관련된 \n",
      "자료가  있습니다 .\n",
      "금  따는  콩밭\n",
      "🙝 🙟 \n",
      "땅속  저  밑은  늘  음침하\n",
      "다 .\n",
      "고달픈  간드렛불 , 맥없이\n",
      "푸르끼하다 .\n",
      "밤과  달라서  낮엔  되우  흐릿하였다 .\n",
      "겉으로  황토  장벽으로  앞뒤좌우가  콕  막힌  좁직한  구뎅이 .\n",
      "흡사히  무덤  속같이  귀중중하다 . 싸늘한  침묵 , 쿠더브레한\n",
      "흙내와  징그러운  냉기만이  그  속에  자욱하다 .\n",
      "곡괭이는  뻔질  흙을  이르집는다 . 암팡스러이  내려쪼며 ,\n",
      "퍽  퍽  퍼억 .\n",
      "이렇게  메떨어진  소리뿐 . 그러나  간간  우수수  하고  벽이  헐\n",
      "린다 .\n",
      "영식이는  일손을  놓고  소맷자락을  끌어당기어  얼굴의  땀을\n",
      "훑는다 . 이놈의  줄이  언제나  잡힐는지  기가  찼다 . 흙  한줌을\n",
      "집어  코밑에  바짝  들여대고  손가락으로  샅샅이  뒤져본다 . 완\n",
      "연히  버력은  좀  변한  듯싶다 . 그러나  불통버력이  아주  다  풀\n",
      "린  것도  아니었다 . 밀똥버력이라야  금이  온다는데  왜  이리\n",
      "안  나오는지 .\n",
      "곡괭이를  다시  집어든다 . 땅에  무릎을  꿇고  궁뎅이를  번쩍\n",
      "든  채  식식거린다 . 곡괭이는  무작정  내려찍는다 . 바닥에서\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c719b391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:05:35+00:00',\n",
       " 'author': 'Unknown',\n",
       " 'moddate': '2024-11-24T07:05:37+00:00',\n",
       " 'title': '금 따는 콩밭',\n",
       " 'source': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'total_pages': 23,\n",
       " 'page': 1,\n",
       " 'page_label': '2'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e295218d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(path)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8689168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "위키백과\n",
      "위키백과에 이 글\n",
      "과 관련된\n",
      "자료가 있습니다.\n",
      "금 따는 콩밭\n",
      "🙝🙟\n",
      "땅속 저 밑은 늘 음침하\n",
      "다.\n",
      "고달픈 간드렛불, 맥없이\n",
      "푸르끼하다.\n",
      "밤과 달라서 낮엔 되우 흐릿하였다.\n",
      "겉으로 황토 장벽으로 앞뒤좌우가 콕 막힌 좁직한 구뎅이.\n",
      "흡사히 무덤 속같이 귀중중하다. 싸늘한 침묵, 쿠더브레한\n",
      "흙내와 징그러운 냉기만이 그 속에 자욱하다.\n",
      "곡괭이는 뻔질 흙\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39b02d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:05:35+00:00',\n",
       " 'source': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'file_path': 'data/novel/금_따는_콩밭_김유정.pdf',\n",
       " 'total_pages': 23,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': '금 따는 콩밭',\n",
       " 'author': 'Unknown',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-11-24T07:05:37+00:00',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20241124070537+00'00'\",\n",
       " 'creationDate': \"D:20241124070535+00'00'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96d909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb331aa",
   "metadata": {},
   "source": [
    "### Web\n",
    "\n",
    "* `WebBaseLoader` \n",
    "\n",
    "* It utilizes `BeautifulSoup` to parse a webdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b69c00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = [\n",
    "    \"https://m.sports.naver.com/wfootball/article/421/0008308548\",\n",
    "    \"https://m.sports.naver.com/wfootball/article/450/0000131435\"\n",
    "]\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path = url,\n",
    "    header_template = {\n",
    "        'user-agent':user_agent\n",
    "    } \n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc515b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://m.sports.naver.com/wfootball/article/421/0008308548',\n",
       " 'title': '쿠팡플레이, 스포츠패스 금액 월 1만 원 확정…15일부터 시행',\n",
       " 'language': 'ko'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ba98fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'손흥민 이름 사라졌다' 유럽 생활 끝내고 사우디 가나...차기 시즌 토트넘 예상 선발 라인업서 제외NAVER스포츠메뉴홈야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어NEW뉴스영상일정순위포토홈 바로가기NAVER스포츠마이팀팀 추가응원하는 팀을 구독해보세요!스포츠야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어콘텐츠오늘의 경기승부예측연재이슈톡대학스포츠랭킹기타고객센터공식 블로그메뉴 닫기본문 바로가기'손흥민 이름 사라졌다' 유럽 생활 끝내고 사우디 가나...차기 시즌 토트넘 예상 선발 라인업서 제외입력2025.06.12. 오후 12:31기사원문강의택 기자공감좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0텍스트 음성 변환 서비스본문 듣기를 종료하였습니다.글자 크기 변경공유하기토트넘 홋스퍼 손흥민. 사진┃게티이미지코리아[STN뉴스] 강의택 기자 = 손흥민(33)이 차기 시즌 토트넘 홋스퍼 예상 선발 라인업에서 제외됐다.영국 매체 '미러'는 11일(한국시각) \"토트넘은 토마스 프랭크 감독을 선임할 예정이다. 이후 새로운 선수들을 영입해 팀을 개편할 것이다\"고 전했다.이어 \"토트넘은 차기 시즌 손흥민의 새로운 경쟁자를 찾아야 한다. 프랭크 감독과 잘 맞을 수 있는 앙투완 세메뇨가 자리를 차지할 수 있다\"고 덧붙였다.그러면서 매체는 차기 시즌 프랭크 감독의 선발 라인업을 예상했는데 왼쪽 윙 자리에 손흥민이 아닌 세메뇨를 넣었다.다음 시즌 토트넘 예상 선발 라인업에서 빠진 손흥민. 사진┃미러토트넘은 직전 시즌부터 공격진 강화를 위해 본머스의 핵심으로 떠오른 세메뇨에게 관심을 보였다.토트넘 커뮤니티 '투 더 레인 앤드 백'은 \"토트넘은 뛰어난 움직임으로 수비수들에게 악몽과도 같은 존재인 세메뇨에게 관심을 보이고 있다\"고 전한 바 있다.손흥민이 차기 시즌 토트넘 예상 선발 라인업에서 빠진 것은 이번이 처음이 아니다. 앞서 영국 매체 '풋볼 인사이더'는 왼쪽 윙어 자리에 에베레치 에제를 넣었다.공교롭게도 최근 손흥민은 여러 이적설에도 휘말리고 있다. 유럽 생활을 끝내고 사우디아라비아 리그로 향할 것이라는 소식이 계속해서 들려오고 있다.영국 매체 '팀토크'는 \"손흥민은 경질된 엔제 포스테코글루 감독과 함께 팀을 떠날 가능성이 있다. 사우디아라비아 리그의 관심을 받고 있다\"고 밝혔다.사우디아라비아 리그의 관심을 받고 있는 손흥민. 사진┃score90우승에도 불구하고 계속해서 나오는 결별설과 이적설은 직전 시즌 손흥민이 보여준 아쉬운 기량이 주된 원인으로 보인다.영국 공영방송 'BBC'는 \"손흥민은 더 이상 토트넘에게 필수적인 선수가 아니다. 예전만큼 빠르고 날카롭지 않다. 영향력은 점점 약해지고 있다\"고 혹평을 남겼다.토트넘 커뮤니티 '스퍼스웹'은 \"손흥민은 마이키 무어에게 자리를 내줘야 한다\"며 충격적인 망언을 내놓기도 했다.기록상으로는 아쉬운 모습을 보인 것이 사실이다. 손흥민은 직전 시즌 리그에서 7골만을 기록하며 2016~2017시즌부터 이어져 온 두 자릿수 득점에 실패했다.토트넘 홋스퍼 캡틴 손흥민. 사진┃게티이미지코리아한편, 토트넘은 2024~2025시즌 프리미어리그(EPL)에서 무려 22패를 기록하며 구단 역사상 한 시즌 최다패라는 불명예 기록을 달성했다. 순위 역시 17위까지 추락했다.이로 인해 토트넘은 포스테코글루 감독을 경질하고 차기 시즌 팀을 이끌 새로운 사령탑 찾기에 나선 상황이다. 현재 브렌트포드의 프랭크 감독이 유력하다.영국 공영방송 'BBC'는 \"토트넘이 새로운 사령탑으로 프랭크 감독을 선임하기 위해 브렌트포드와 협상을 시작했다\"고 전했다.STN뉴스=강의택 기자강의택 기자구독구독자 0응원수 0'김민재 반 다이크 듀오 보나' EPL 챔피언 리버풀 이적설 \"센터백 영입 보강 계획...관심 표명 중\"'손흥민 어디 갔나요?' 차기 시즌 토트넘 예상 선발 라인업서 제외 \"팰리스 에이스가 대체할 것\"Copyright ⓒ STN 스포츠. All rights reserved. 무단 전재 및 재배포 금지.기사 섹션 분류 가이드기사 섹션 분류 안내스포츠 기사 섹션(종목) 정보는 언론사 분류와 기술 기반의 자동 분류 시스템을 따르고 있습니다. 오분류에 대한 건은 네이버스포츠로 제보 부탁드립니다.오분류 제보하기닫기STN NEWS 모바일 구독!스포츠 브랜드의 모든것 리보니스!주요뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다.박주현 맞아? 어마어마한 글래머 '깜짝' 진짜 美쳤다!'박명수 발탁' 女배우, 파격 가슴 노출…속옷도 안 입고'전라 노출' 박지현, 또 파격 노출…글래머+초섹시제니, 상반신 누드급 노출…딱 가슴만 겨우 가렸네!치어리더 우수한, '글래머'도 초특급 우수하네!좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0기사 공유하기STN 스포츠 언론사홈 바로가기\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse only a part of a certain page\n",
    "# BeautifulSoup: SoupStrainer\n",
    "# BeautifulSoup(\"html document\", parse_only = StrainerObject)\n",
    "#    L Find only in the Area StrainerObject initialize.\n",
    "# SoupStrainer(\"TagName\") -> Only in the TagNames\n",
    "# SoupStrainer(\"TagName\", attrs = {\"attributeName\":\"attributeValue\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d2f4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path = url,\n",
    "    bs_kwargs = {\n",
    "        \"parse_only\":bs4.SoupStrainer(attrs={\"class\":\"_article_content\"})\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "# div content = \"article_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60c830d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿠팡 와우 회원, 월 총 요금  '1만 7890원'(쿠팡플레이 갈무리)/뉴스1(서울=뉴스1) 김정현 양새롬 기자 = 쿠팡플레이가 부가서비스인 '스포츠 패스'의 금액을 월 1만 원으로 확정했다.12일 업계에 따르면 쿠팡플레이는 오는 15일 해외 스포츠 등의 콘텐츠를 유료 부가 서비스로 제공하는 스포츠 패스의 요금을 월 1만 원으로 결정했다. 쿠팡 와우 멤버십 구독료인 월 7890원에 스포츠패스 금액을 더하면 월 이용금액은 1만 7890원이 된다.이번 패스를 통해 볼수 있는 스포츠 리그는 △FIFA대회(FIFA클럽월드컵)△유럽축구리그(프리미어리그 2025~2026 시즌, 라리가, 분데스리가, 분데스리가2, 리그1, EFL 챔피언십 EFL리그원, 에레디비시) △유럽축구 토너먼트(FA컵, 카라바오컵, 커뮤니티쉴드, 코파 델레이, 수페르코파데 에스파냐, DFB-포칼, DFL-슈퍼컵, 쿠프드프랑스, 트로페데 샹피옹, 버투트로피) △아시아축구(AFC아시안컵, AFC챔피언스리그 엘리트, AFC챔피언스리그2, 기타AFC주관 국제 대회) △세계축구(월드컵남미 예선, 클럽 친선경기, 해외 국가 친선경기) 등이다.앞서 쿠팡플레이는 선택형 부가서비스 '패스(PASS)'를 6월 중 도입한다고 밝힌 바 있다.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad5b3e",
   "metadata": {},
   "source": [
    "### ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fed3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'itertools.islice'>\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"RAG\",\n",
    "    max_results = 2,\n",
    "    sort_by = arxiv.SortCriterion.Relevance  # cf. Relevance,  LastUpdatedDate, SubmittedDate\n",
    ")\n",
    "client = arxiv.Client()\n",
    "result = client.results(search)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202a0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class islice in module itertools:\n",
      "\n",
      "class islice(builtins.object)\n",
      " |  islice(iterable, stop) --> islice object\n",
      " |  islice(iterable, start, stop[, step]) --> islice object\n",
      " |\n",
      " |  Return an iterator whose next() method returns selected values from an\n",
      " |  iterable.  If start is specified, will skip all preceding elements;\n",
      " |  otherwise, start defaults to zero.  Step defaults to one.  If\n",
      " |  specified as another value, step determines how many values are\n",
      " |  skipped between successive calls.  Works like a slice() on a list\n",
      " |  but returns an iterator.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |\n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |\n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |\n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |\n",
      " |  __setstate__(...)\n",
      " |      Set state information for unpickling.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __new__(*args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "help(islice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceeb0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = next(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d36923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "Authors: [arxiv.Result.Author('Yunfan Gao'), arxiv.Result.Author('Yun Xiong'), arxiv.Result.Author('Meng Wang'), arxiv.Result.Author('Haofen Wang')]\n",
      "Summary: Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "File URL: http://arxiv.org/pdf/2407.21059v1\n"
     ]
    }
   ],
   "source": [
    "print(\"Title:\", doc1.title)\n",
    "print(\"Authors:\", doc1.authors)\n",
    "print(\"Summary:\", doc1.summary)\n",
    "print(\"File URL:\", doc1.pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b677eec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'papers\\\\Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.makedirs(\"papers\")\n",
    "doc1.download_pdf(\"papers\", f\"{doc1.title}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdba0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "\n",
    "# load_max_docs\n",
    "loader = ArxivLoader(\n",
    "    query = \"Advanced RAG\",\n",
    "    top_k_results = 3\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486071cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-07-26',\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
       " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5e77a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, Advanced\n",
      "RAG paradigm focuses on optimizing the retrieval phase,\n",
      "aiming to enhance retrieval efficiency and strengthen the\n",
      "utilization of retrieved chunks. As shown in Figure 1 ,typical\n",
      "strategies involve pre-retrieval processing and post-retrieval\n",
      "processing. For instance, query rewriting is used to make\n",
      "the queries more clear and specific, thereby increasing the\n",
      "accuracy of retrieval [10], and the reranking of retrieval results\n",
      "is employed to enhance the LLM’s ability to identify and\n",
      "utilize key information [11].\n",
      "Despite the improvements in the practicality of Advanced\n",
      "RAG, there remains a gap between its capabilities and real-\n",
      "world application requirements. On one hand, as RAG tech-\n",
      "nology advances, user expectations rise, demands continue to\n",
      "evolve, and application settings become more complex. For\n",
      "instance, the integration of heterogeneous data and the new\n",
      "demands for system transparency, control, and maintainability.\n",
      "On the other hand, the growth in application demands has\n",
      "further propelled the evolution of RAG technology.\n",
      "As shown in Figure 2, to achieve more accurate and efficient\n",
      "task execution, modern RAG systems are progressively inte-\n",
      "grating more sophisticated function, such as organizing more\n",
      "refined index base in the form of knowledge graphs, integrat-\n",
      "ing structured data through query construction methods, and\n",
      "employing fine-tuning techniques to enable encoders to better\n",
      "adapt to domain-specific documents.\n",
      "In terms of process design, the current RAG system has\n",
      "surpassed the traditional linear retrieval-generation paradigm.\n",
      "Researchers use iterative retrieval [12] to obtain richer con-\n",
      "text, recursive retrieval [13] to handle complex queries, and\n",
      "adaptive retrieval [14] to provide overall autonomy and flex-\n",
      "ibility. This flexibility in the process significantly enhances\n",
      "arXiv:2407.21059v1  [cs.CL]  26 Jul 2024\n",
      "2\n",
      "Fig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\n",
      "questions, both encounter limitations and struggle to provide satisfactory\n",
      "answers. Despite the fact that Advanced RAG improves retrieval accuracy\n",
      "through hierarchical indexing, pre-retrieval, and post-retrieval processes, these\n",
      "relevant documents have not been used correctly.\n",
      "the expressive power and adaptability of RAG systems, en-\n",
      "abling them to better adapt to various application scenarios.\n",
      "However, this also makes the orchestration and scheduling of\n",
      "workflows more complex, posing greater challenges to system\n",
      "design. Specifically, RAG currently faces the following new\n",
      "challenges:\n",
      "Complex data sources integration. RAG are no longer\n",
      "confined to a single type of unstructured text data source but\n",
      "have expanded to include various data types, such as semi-\n",
      "structured data like tables and structured data like knowledge\n",
      "graphs [15]. Access to heterogeneous data from multiple\n",
      "sources can provide the system with a richer knowledge\n",
      "background, and more reliable knowledge verification capa-\n",
      "bilities [16].\n",
      "New demands for system interpretability, controllability,\n",
      "Fig. 2.\n",
      "Case of current Modular RAG.The system integrates diverse data\n",
      "and more functional components. The process is no longer confined to linear\n",
      "but is controlled by multiple control components for retrieval and generation,\n",
      "making the entire system more flexible and complex.\n",
      "3\n",
      "and maintainability. With the increasing complexity of sys-\n",
      "tems, system maintenance and debugging have become more\n",
      "challenging. Additionally, when issues arise, it is essential to\n",
      "quickly pinpoint the specific components that require opti-\n",
      "mization.\n",
      "Component selection and optimization. More neural net-\n",
      "works are involved in the RAG system, necessitating the\n",
      "selection of appropriate components to meet the needs of spe-\n",
      "cific tasks and resource configurations. Moreover, additional\n",
      "components enhance the effectiveness of RAG but also bring\n",
      "new collaborative work requirements [17]. Ensuring that these\n",
      "models perform as intended and work efficiently together to\n",
      "enhance the overall system performance is crucial.\n",
      "Workflow orchestration and scheduling. Components\n",
      "may need to be executed in a specific order, processed in paral-\n",
      "lel under certain conditions, or even judged by the LLM based\n",
      "on different outputs. Reasonable planning of the workflow is\n",
      "essential for improving system efficiency and achieving the\n",
      "desired outcomes [18].\n",
      "To address the design, management, and maintenance chal-\n",
      "lenges posed by the increasing complexity of RAG systems,\n",
      "and to meet the ever-growing and diverse demands and ex-\n",
      "pectations, this paper proposes Modular RAG architecture.\n",
      "In modern computing systems, modularization is becoming\n",
      "a trend. It can enhance the system’s scalability and maintain-\n",
      "ability and achieve efficient task execution through process\n",
      "control.\n",
      "The Modular RAG system consists of multiple independent\n",
      "yet tightly coordinated modules, each responsible for handling\n",
      "specific functions or tasks. This architecture is divided into\n",
      "three levels: the top level focuses on the critical stages of\n",
      "RAG, where each stage is treated as an independent module.\n",
      "This level not only inherits the main processes from the\n",
      "Advanced RAG paradigm but also introduces an orchestration\n",
      "module to control the coordination of RAG processes. The\n",
      "middle level is composed of sub-modules within each module,\n",
      "further refining and optimizing the functions. The bottom level\n",
      "consists of basic units of operation—operators. Within the\n",
      "Modular RAG framework, RAG systems can be represented\n",
      "in the form of computational graphs, where nodes represent\n",
      "specific operators. The comparison of the three paradigms is\n",
      "shown in the Figure 3. Modular RAG evolves based on the\n",
      "previous development of RAG. The relationships among these\n",
      "three paradigms are ones of inheritance and development.\n",
      "Advanced RAG is a special case of Modular RAG, while Naive\n",
      "RAG is a special case of Advanced RAG.\n",
      "The advantages of Modular RAG are significant, as it\n",
      "enhances the flexibility and scalability of RAG systems. Users\n",
      "can flexibly combine different modules and operators accord-\n",
      "ing to the requirements of data sources and task scenarios. In\n",
      "summary, the contributions of this paper are as follows:\n",
      "• This paper proposes a new paradigm called modular\n",
      "RAG, which employs a three-tier architectural design\n",
      "comprising modules, sub-modules, and operators to de-\n",
      "fine the RAG system in a unified and structured manner.\n",
      "This design not only enhances the system’s flexibility and\n",
      "scalability but also, through the independent design of\n",
      "operators, strengthens the system’s maintainability and\n",
      "comprehensibility.\n",
      "• Under the framework of Modular RAG, the orchestration\n",
      "of modules and operators forms the RAG Flow, which\n",
      "can flexibly express current RAG methods. This paper has\n",
      "further summarized six typical flow patterns and specific\n",
      "methods have been analyzed to reveal the universality of\n",
      "modular RAG in practical scenarios.\n",
      "• The Modular RAG framework offers exceptional flexi-\n",
      "bility and extensibility. This paper delves into the new\n",
      "opportunities brought by Modular RAG and provides a\n",
      "thorough discussion on the adaptation and expansion of\n",
      "new methods in different application scenarios, offering\n",
      "guidance for future research directions and practical ex-\n",
      "ploration.\n",
      "II. RELATED WORK\n",
      "The development of RAG technology can be summarized\n",
      "in three stages. Initially, retrieval-augmented techniques were\n",
      "introduced to improve the performance of pre-trained lan-\n",
      "guage models on knowledge-intensive tasks [19], [20]. In\n",
      "specific implementations, Retro [21] optimized pre-trained\n",
      "autoregressive models through retrieval augmentation, while\n",
      "Atlas [22] utilized a retrieval-augmented few-shot fine-tuning\n",
      "method, enabling language models to adapt to diverse tasks.\n",
      "IRCOT [23] further enriched the reasoning process during\n",
      "the inference phase by combining chain-of-thought and multi-\n",
      "step retrieval processes. Entering the second stage, as the\n",
      "language processing capabilities of LLMs significantly im-\n",
      "proved, retrieval-augmented techniques began to serve as a\n",
      "means of supplementing additional knowledge and providing\n",
      "references, aiming to reduce the hallucination. For instance,\n",
      "RRR [24] improved the rewriting phase, and LLMlingua [25]\n",
      "removed redundant tokens in retrieved document chunks.\n",
      "With the continuous progress of RAG technology, research\n",
      "has become more refined and focused, while also achieving\n",
      "innovative integration with other technologies such as graph\n",
      "neural networks [26] and fine-tuning techniques [27]. The\n",
      "overall pipeline has also become more flexible, such as using\n",
      "LLMs to proactively determine the timing of retrieval and\n",
      "generation [14], [28].\n",
      "The development of RAG technology has been acceler-\n",
      "ated by LLM technology and practical application needs.\n",
      "Researchers are examining and organizing the RAG frame-\n",
      "work and development pathways from different perspectives.\n",
      "Building upon the enhanced stages of RAG, Gao et al., [2] sub-\n",
      "divided RAG into enhancement during pre-training, inference,\n",
      "and fine-tuning stages. Based on the main processes of RAG,\n",
      "relevant works on RAG were organized from the perspectives\n",
      "of retrieval, generation, and augmentation methods. Huang\n",
      "et al., [29] categorize RAG methods into four main classes:\n",
      "pre-retrieval, retrieval, post-retrieval, generation, and provide\n",
      "a detailed discussion of the methods and techniques within\n",
      "each class. Hu et al., [30] discuss Retrieval-Augmented Lan-\n",
      "guage Models (RALMs) form three key components, including\n",
      "retrievers, language models, augmentations, and how their\n",
      "interactions lead to different model structures and applications.\n",
      "4\n",
      "Fig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\n",
      "systems.\n",
      "They emphasize the importance of considering robustness,\n",
      "accuracy, and relevance when evaluating RALMs and pro-\n",
      "pose several evaluation methods. Ding et al., [31] provide a\n",
      "comprehensive review from the perspectives of architecture,\n",
      "training strategies, and applications. They specifically discuss\n",
      "four training methods of RALMs: training-free methods, in-\n",
      "dependent training methods, sequence training methods, and\n",
      "joint training methods, and compare their advantages and\n",
      "disadvantages. Zhao et al., [32]analyze the applications of\n",
      "RAG technology in various fields such as text generation,\n",
      "code generation, image generation, and video generation from\n",
      "the perspective of augmented intelligence with generative\n",
      "capabilities.\n",
      "The current collation of RAG systems primarily focuses\n",
      "on methods with a fixed process, mainly concerned with\n",
      "optimizing the retrieval and generation stages. However, it has\n",
      "not turned its attention to the new characteristics that RAG\n",
      "research is continuously evolving, namely the characteristics\n",
      "of process scheduling and functional componentization. There\n",
      "is currently a lack of comprehensive analysis of the overall\n",
      "RAG system, which has led to research on paradigms lagging\n",
      "behind the development of RAG technology.\n",
      "III. FRAMEWORK AND NOTATION\n",
      "For query Q = {qi}, a typical RAG system mainly consists\n",
      "of three key components. 1) Indexing. Given documents D =\n",
      "{d1, d2, . . . , dn} , where di represents the document chunk.\n",
      "Indexing is the process of converting di into vectors through\n",
      "an embedding model fe(·) , and then store vectors in vector\n",
      "database.\n",
      "I = {e1, e2, . . . , en}\n",
      "and\n",
      "ei = fe(di) ∈Rd\n",
      "(1)\n",
      "Notation\n",
      "Description\n",
      "q\n",
      "The original query\n",
      "y\n",
      "The output of LLM\n",
      "D\n",
      "A document retrieval repository composed of chunks di.\n",
      "R(q, D)\n",
      "Retriever,find similar chunks from D based on q.\n",
      "F\n",
      "RAG Flow\n",
      "P\n",
      "RAG Flow pattern\n",
      "fqe\n",
      "Query expansion function\n",
      "fqc\n",
      "Query transform function\n",
      "fcomp\n",
      "Chunk compression function\n",
      "fsel\n",
      "Chunk selection function\n",
      "fr\n",
      "Routing function\n",
      "M\n",
      "Module in modular RAG\n",
      "op\n",
      "The specific operators within the Module.\n",
      "TABLE I\n",
      "IMPORTANT NOTATION\n",
      "2) Retrieval . Transform the query into a vector using the\n",
      "same encoding model, and then filter out the top k document\n",
      "chunks that are most similar based on vector similarity.\n",
      "R : topk\n",
      "di∈D\n",
      "Sim(q, di) →Dq\n",
      "(2)\n",
      "Dq = {d1, d2, . . . , dk} represents the relevant documents for\n",
      "question q. The similarity function Sim(·) commonly used are\n",
      "dot product or cosine similarity.\n",
      "Sim(q, di) = eq · edi\n",
      "or\n",
      "eq · edi\n",
      "∥eq∥· ∥edi∥\n",
      "(3)\n",
      "3) Generation. After getting the relevant documents. The\n",
      "query q and the retrieved document Dq chunks are inputted\n",
      "together to the LLM to generate the final answer, where [·, ·]\n",
      "stands for concatenation.\n",
      "y = LLM([Dq, q])\n",
      "(4)\n",
      "5\n",
      "With the evolution of RAG technology, more and more func-\n",
      "tional components are being integrated into systems. Modular\n",
      "RAG paradigm includes three levels, ranging from large to\n",
      "small:\n",
      "L1 Module (M = {Ms}). The core process in RAG\n",
      "system.\n",
      "L2 Sub-module (Ms = {Op}).The functional modules in\n",
      "module.\n",
      "L3 Operator (Op = {fθi}). The the specific functional\n",
      "implementation in a module or sub-module. As a result, a\n",
      "Modular RAG system can be represented as:\n",
      "G = {q, D, M, {Ms}, {Op}}\n",
      "(5)\n",
      "The arrangement between modules and operators constitutes\n",
      "the RAG Flow F = (Mϕ1, . . . , Mϕn) where ϕ stands for\n",
      "the set of module parameters. A modular rag flow can be\n",
      "decomposed into a graph of sub-functions. In the simplest\n",
      "case,the graph is a linear chain.\n",
      "NaiveRAG : q\n",
      "R(q,D)\n",
      "−−−−−−−−−−−→\n",
      "T ext−Embedding Dq\n",
      "LLM([q,Dq])\n",
      "−−−−−−−−−−−→\n",
      "OpenAI/GP T −4 y\n",
      "(6)\n",
      "IV. MODULE AND OPERATOR\n",
      "This chapter will specifically introduce modules and op-\n",
      "erators under the Modular RAG framework. Based on the\n",
      "current stage of RAG development, we have established\n",
      "six main modules: Indexing, Pre-retrieval, Retrieval, Post-\n",
      "retrieval, Generation, and Orchestration.\n",
      "A. Indexing\n",
      "Indexing is the process of split document into manageable\n",
      "chunks and it is a key step in organizing a system. Indexing\n",
      "faces three main challenges. 1) Incomplete content represen-\n",
      "tation.The semantic information of chunks is influenced by the\n",
      "segmentation method, resulting in the loss or submergence of\n",
      "important information within longer contexts. 2) Inaccurate\n",
      "chunk similarity search. As data volume increases, noise in\n",
      "retrieval grows, leading to frequent matching with erroneous\n",
      "data, making the retrieval system fragile and unreliable. 3)\n",
      "Unclear reference trajectory. The retrieved chunks may orig-\n",
      "inate from any document, devoid of citation trails, potentially\n",
      "resulting in the presence of chunks from multiple different\n",
      "documents that, despite being semantically similar, contain\n",
      "content on entirely different topics.\n",
      "1) Chunk Optimization: The size of the chunks and the\n",
      "overlap between the chunks play a crucial role in the overall\n",
      "effectiveness of the RAG system. Given a chunk di, its chunk\n",
      "size is denoted as Li = |di|, and the overlap is denoted as\n",
      "Lo\n",
      "i = |di ∩di+1|. Larger chunks can capture more context,\n",
      "but they also generate more noise, requiring longer processing\n",
      "time and higher costs. While smaller chunks may not fully\n",
      "convey the necessary context, they do have less noise [17].\n",
      "Sliding Window using overlapping chunks in a sliding win-\n",
      "dow enhances semantic transitions. However, it has limitations\n",
      "such as imprecise context size control, potential truncation of\n",
      "words or sentences, and lacking semantic considerations.\n",
      "Metadata Attachment. Chunks can be enriched with meta-\n",
      "data like page number, file name, author, timestamp, sum-\n",
      "mary, or relevant questions. This metadata allows for filtered\n",
      "retrieval, narrowing the search scope.\n",
      "Small-to-Big [33] separate the chunks used for retrieval\n",
      "from those used for synthesis. Smaller chunks enhance re-\n",
      "trieval accuracy, while larger chunks provide more context.\n",
      "One approach is to retrieve smaller summarized chunks and\n",
      "reference their parent larger chunks. Alternatively, individual\n",
      "sentences could be retrieved along with their surrounding text.\n",
      "2) Structure Organization: One effective method for en-\n",
      "hancing information retrieval is to establish a hierarchical\n",
      "structure for the documents. By constructing chunks structure,\n",
      "RAG system can expedite the retrieval and processing of\n",
      "pertinent data.\n",
      "Hierarchical Index. In the hierarchical structure of docu-\n",
      "ments, nodes are arranged in parent-child relationships, with\n",
      "chunks linked to them. Data summaries are stored at each\n",
      "node, aiding in the swift traversal of data and assisting the\n",
      "RAG system in determining which chunks to extract. This\n",
      "approach can also mitigate the illusion caused by chunk\n",
      "extraction issues. The methods for constructing a structured\n",
      "index primarily include: 1) Structural awareness based on\n",
      "paragraph and sentence segmentation in docs. 2) Content\n",
      "awareness based on inherent structure in PDF, HTML, and\n",
      "Latex. 3) Semantic awareness based on semantic recognition\n",
      "and segmentation of text.\n",
      "KG Index [34]. Using Knowledge Graphs (KGs) to struc-\n",
      "ture documents helps maintain consistency by clarifying con-\n",
      "nections between concepts and entities, reducing the risk of\n",
      "mismatch errors. KGs also transform information retrieval\n",
      "into instructions intelligible to language models, improving re-\n",
      "trieval accuracy and enabling contextually coherent responses.\n",
      "This enhances the overall efficiency of the RAG system.\n",
      "For example, organizing a corpus in the format of graph\n",
      "G = {V, E, X}, where node V = {vi}n\n",
      "i=1 represent document\n",
      "structures (e.g.passage, pages, table) , edge E ⊂V × V rep-\n",
      "resent semantic or lexical similarity and belonging relations,\n",
      "and node features X = {Xi}n\n",
      "i=1 represent text or markdown\n",
      "content for passage.\n",
      "B. Pre-retrieval\n",
      "One of the primary challenges with Naive RAG is its\n",
      "direct reliance on the user’s original query as the basis for\n",
      "retrieval. Formulating a precise and clear question is difficult,\n",
      "and imprudent queries result in subpar retrieval effectiveness.\n",
      "The primary challenges in this module include: 1) Poorly\n",
      "worded queries. The question itself is complex, and the\n",
      "language is not well-organized. 2) Language complexity and\n",
      "ambiguity. Language models often struggle when dealing\n",
      "with specialized vocabulary or ambiguous abbreviations with\n",
      "multiple meanings. For instance, they may not discern whether\n",
      "LLM refers to Large Language Model or a Master of Laws in\n",
      "a legal context.\n",
      "1) Query Expansion : Expanding a single query into mul-\n",
      "tiple queries enriches the content of the query, providing\n",
      "6\n",
      "further context to address any lack of specific nuances, thereby\n",
      "ensuring the optimal relevance of the generated answers.\n",
      "fqe(q) = {q1, q2, . . . , qn}\n",
      "∀qi ∈{q1, q2, . . . , qn}, qi /∈Q\n",
      "(7)\n",
      "Multi-Query uses prompt engineering to expand queries\n",
      "via LLMs, allowing for parallel execution. These expansions\n",
      "are meticulously designed to ensure diversity and coverage.\n",
      "However, this approach can dilute the user’s original intent.\n",
      "To mitigate this, the model can be instructed to assign greater\n",
      "weight to the original query.\n",
      "Sub-Query. By decomposing and planning for complex\n",
      "problems, multiple sub-problems are generated. Specifically,\n",
      "least-to-most prompting [35] can be employed to decom-\n",
      "pose the complex problem into a series of simpler sub-\n",
      "problems. Depending on the structure of the original problem,\n",
      "the generated sub-problems can be executed in parallel or\n",
      "sequentially. Another approach involves the use of the Chain-\n",
      "of-Verification (CoVe) [36]. The expanded queries undergo\n",
      "validation by LLM to achieve the effect of reducing hallu-\n",
      "cinations.\n",
      "2) Query Transformation: Retrieve and generate based on\n",
      "a transformed query instead of the user’s original query.\n",
      "fqt(q) = q′\n",
      "(8)\n",
      "Rewrite. Original queries often fall short for retrieval in\n",
      "real-world scenarios. To address this, LLMs can be prompted\n",
      "to rewrite. Specialized smaller models can also be employed\n",
      "for this purpose [24]. The implementation of the query rewrite\n",
      "method in Taobao has significantly improved recall effective-\n",
      "ness for long-tail queries, leading to an increase in GMV [10].\n",
      "HyDE [37]. In order to bridge the semantic gap between\n",
      "questions and answers, it constructs hypothetical documents\n",
      "(assumed answers) when responding to queries instead of\n",
      "directly searching the query. It focuses on embedding simi-\n",
      "larity from answer to answer rather than seeking embedding\n",
      "similarity for the problem or query. In addition, it also in-\n",
      "cludes reverse HyDE, which generate hypothetical query for\n",
      "each chunks and focuses on retrieval from query to query.\n",
      "Step-back Prompting [38]. The original query is abstracted\n",
      "into a high-level concept question (step-back question). In the\n",
      "RAG system, both the step-back question and the original\n",
      "query are used for retrieval, and their results are combined\n",
      "to generate the language model’s answer.\n",
      "3) Query Construction: In addition to text data, an in-\n",
      "creasing amount of structured data, such as tables and graph\n",
      "data, is being integrated into RAG systems. To accommodate\n",
      "various data types, it is necessary to restructure the user’s\n",
      "query. This involve converting the query into another query\n",
      "language to access alternative data sources, with common\n",
      "methods including Text-to-SQL or Text-to-Cypher . In many\n",
      "scenarios, structured query languages (e.g., SQL, Cypher)\n",
      "are often used in conjunction with semantic information and\n",
      "metadata to construct more complex queries.\n",
      "fqc(q) = q∗, q∗∈Q∗= {SQL, Cypher, . . . }\n",
      "(9)\n",
      "C. Retrieval\n",
      "The retrieval process is pivotal in RAG systems. By lever-\n",
      "aging powerful embedding models, queries and text can be\n",
      "efficiently represented in latent spaces, which facilitates the\n",
      "establishment of semantic similarity between questions and\n",
      "documents, thereby enhancing retrieval. Three main consider-\n",
      "ations that need to be addressed include retrieval efficiency,\n",
      "quality, and the alignment of tasks, data and models.\n",
      "1) Retriever Selection: With the widespread adoption of\n",
      "RAG technology, the development of embedding models has\n",
      "been in full swing. In addition to traditional models based\n",
      "on statistics and pre-trained models based on the encoder\n",
      "structure, embedding models fine-tuned on LLMs have also\n",
      "demonstrated powerful capabilities [39]. However, they often\n",
      "come with more parameters, leading to weaker inference\n",
      "and retrieval efficiency. Therefore, it is crucial to select the\n",
      "appropriate retriever based on different task scenarios.\n",
      "Sparse Retriever uses statistical methods to convert queries\n",
      "and documents into sparse vectors. Its advantage lies in its\n",
      "efficiency in handling large datasets, focusing only on non-zero\n",
      "elements. However, it may be less effective than dense vectors\n",
      "in capturing complex semantics. Common methods include\n",
      "TF-IDF and BM25.\n",
      "Dense Retriever employs pre-trained language models\n",
      "(PLMs) to provide dense representations of queries and doc-\n",
      "uments. Despite higher computational and storage costs, it\n",
      "offers more complex semantic representations. Typical models\n",
      "include BERT structure PLMs, like ColBERT, and multi-task\n",
      "fine-tuned models like BGE [40] and GTE [41].\n",
      "Hybrid Retriever is to use both sparse and dense retrievers\n",
      "simultaneously. Two embedding techniques complement each\n",
      "other to enhance retrieval effectiveness. Sparse retriever can\n",
      "provide initial screening results. Additionally, sparse models\n",
      "enhance the zero-shot retrieval capabilities of dense models,\n",
      "particularly in handling queries with rare entities, thereby\n",
      "increasing system robustness.\n",
      "2) Retriever Fine-tuning: In cases where the context may\n",
      "diverge from pre-trained corpus, particularly in highly special-\n",
      "ized fields like healthcare, law, and other domains abundant in\n",
      "proprietary terminology. While this adjustment demands addi-\n",
      "tional effort, it can substantially enhance retrieval efficiency\n",
      "and domain alignment.\n",
      "Supervised Fine-Tuning (SFT). Fine-tuning a retrieval\n",
      "model based on labeled domain data is typically done using\n",
      "contrastive learning. This involves reducing the distance be-\n",
      "tween positive samples while increasing the distance between\n",
      "negative samples. The commonly used loss calculation is\n",
      "shown in the following:\n",
      "L(DR) = −1\n",
      "T\n",
      "T\n",
      "X\n",
      "i=1\n",
      "log\n",
      "e(sim(qi,d+\n",
      "i ))\n",
      "e(sim(qi,d+\n",
      "i )) + PN\n",
      "j=1 e(sim(qi,d−\n",
      "i ))\n",
      "(10)\n",
      "where d+\n",
      "i is the positive sample document corresponding to\n",
      "the i-th query, d−\n",
      "i\n",
      "is several negative sample, T is the total\n",
      "number of queries, N is the number of negative samples, and\n",
      "DR is the fine-tuning dataset.\n",
      "LM-supervised Retriever (LSR). In contrast to directly\n",
      "constructing a fine-tuning dataset from the dataset, LSR uti-\n",
      "7\n",
      "lizes the LM-generated results as supervisory signals to fine-\n",
      "tune the embedding model during the RAG process.\n",
      "PLSR(d|q, y) =\n",
      "ePLM(y|d,q)/β\n",
      "P\n",
      "d′∈D ePLM(y|d,q)/β)\n",
      "(11)\n",
      "PLM(y|d, q) is LM probability of the ground truth output y\n",
      "given the input context d and query q, and β is a hyper-\n",
      "paramter.\n",
      "Adapter. At times, fine-tuning a large retriever can be\n",
      "costly, especially when dealing with retrievers based on LLMs\n",
      "like gte-Qwen. In such cases, it can mitigate this by incorpo-\n",
      "rating an adapter module and conducting fine-tuning. Another\n",
      "benefit of adding an adapter is the ability to achieve better\n",
      "alignment with specific downstream tasks [42].\n",
      "D. Post-retrieval\n",
      "Feeding all retrieved chunks directly into the LLM is not an\n",
      "optimal choice. Post-processing the chunks can aid in better\n",
      "leveraging the contextual information. The primary challenges\n",
      "include: 1) Lost in the middle. Like humans, LLM tends\n",
      "to remember only the beginning or the end of long texts,\n",
      "while forgetting the middle portion [43]. 2) Noise/anti-fact\n",
      "chunks. Retrieved noisy or factually contradictory documents\n",
      "can impact the final retrieval generation [44].\n",
      "3) Context\n",
      "Window. Despite retrieving a substantial amount of relevant\n",
      "content, the limitation on the length of contextual information\n",
      "in large models prevents the inclusion of all this content.\n",
      "1) Rerank: Rerank the retrieved chunks without altering\n",
      "their content or length, to enhance the visibility of the more\n",
      "crucial document chunks. Given the retrieved set Dq and a\n",
      "re-ranking method frerank to obtain the re-ranked set:\n",
      "Dq\n",
      "r = frerank(q, Dq) = {d′\n",
      "1, d′\n",
      "2, . . . , d′\n",
      "k}\n",
      "wheref(d′\n",
      "1) ≥f(d′\n",
      "2) ≥. . . ≥f(d′\n",
      "k).\n",
      "(12)\n",
      "Rule-base rerank. Metrics are calculated to rerank chunks\n",
      "according to certain rules. Common metrics include: diversity,\n",
      "relevance and MRR (Maximal Marginal Relevance) [45]. The\n",
      "idea is to reduce redundancy and increase result diversity.\n",
      "MMR selects phrases for the final key phrase list based on a\n",
      "combined criterion of query relevance and information novelty.\n",
      "Model-base rerank. Utilize a language model to reorder the\n",
      "document chunks, commonly based on the relevance between\n",
      "the chunks and the query. Rerank models have become an\n",
      "important component of RAG systems, and relevant model\n",
      "technologies are also being iteratively upgraded. The scope\n",
      "reordering has also been extended to multimodal data such as\n",
      "tables and images [46].\n",
      "2) Compression: A common misconception in the RAG\n",
      "process is the belief that retrieving as many relevant docu-\n",
      "ments as possible and concatenating them to form a lengthy\n",
      "retrieval prompt is beneficial. However, excessive context can\n",
      "introduce more noise, diminishing the LLM’s perception of\n",
      "key information. A common approach to address this is to\n",
      "compress and select the retrieved content.\n",
      "Dq\n",
      "c = fcomp(q, Dq),\n",
      "where|dqc\n",
      "i | < |dq\n",
      "i |\n",
      "∀dq\n",
      "i ∈Dq\n",
      "(13)\n",
      "(Long)LLMLingua [47]. By utilizing aligned and trained\n",
      "small language models, such as GPT-2 Small or LLaMA-\n",
      "7B, the detection and removal of unimportant tokens from\n",
      "the prompt is achieved, transforming it into a form that is\n",
      "challenging for humans to comprehend but well understood by\n",
      "LLMs. This approach presents a direct and practical method\n",
      "for prompt compression, eliminating the need for additional\n",
      "training of LLMs while balancing language integrity and\n",
      "compression ratio.\n",
      "3) Selection: Unlike compressing the content of document\n",
      "chunks, Selection directly removes irrelevant chunks.\n",
      "Dq\n",
      "s = fsel(Dq) = {di ∈D | ¬P(di)}\n",
      "(14)\n",
      "Where fsel is the function for deletion operation and P(di) is\n",
      "a conditional predicate indicating that document (di) satisfies\n",
      "a certain condition. If document (di) satisfies (P(di)), it will\n",
      "be deleted. Conversely, documents for which (¬P(di)) is true\n",
      "will be retained.\n",
      "Selective Context. By identifying and removing redundant\n",
      "content in the input context, the input is refined, thus improv-\n",
      "ing the language model’s reasoning efficiency. In practice, se-\n",
      "lective context assesses the information content of lexical units\n",
      "based on the self-information computed by the base language\n",
      "model. By retaining content with higher self-information, this\n",
      "method offers a more concise and efficient textual representa-\n",
      "tion, without compromising their performance across diverse\n",
      "applications. However, it overlooks the interdependence be-\n",
      "tween compressed content and the alignment between the\n",
      "targeted language model and the small language model utilized\n",
      "for prompting compression [48].\n",
      "LLM-Critique. Another straightforward and effective ap-\n",
      "proach involves having the LLM evaluate the retrieved content\n",
      "before generating the final answer. This allows the LLM\n",
      "to filter out documents with poor relevance through LLM\n",
      "critique. For instance, in Chatlaw [49], the LLM is prompted\n",
      "to self-suggestion on the referenced legal provisions to assess\n",
      "their relevance.\n",
      "E. Generation\n",
      "Utilize the LLM to generate answers based on the user’s\n",
      "query and the retrieved contextual information. Select an\n",
      "appropriate model based on the task requirements, considering\n",
      "factors such as the need for fine-tuning, inference efficiency,\n",
      "and privacy protection.\n",
      "1) Generator Fine-tuning: In addition to direct LLM usage,\n",
      "targeted fine-tuning based on the scenario and data character-\n",
      "istics can yield better results. This is also one of the greatest\n",
      "advantages of using an on-premise setup LLMs.\n",
      "Instruct-Tuning. When LLMs lack data in a specific do-\n",
      "main, additional knowledge can be provided to the LLM\n",
      "through fine-tuning. General fine-tuning dataset can also be\n",
      "used as an initial step. Another benefit of fine-tuning is the\n",
      "ability to adjust the model’s input and output. For example, it\n",
      "can enable LLM to adapt to specific data formats and generate\n",
      "responses in a particular style as instructed [50].\n",
      "Reinforcement learning. Aligning LLM outputs with hu-\n",
      "man or retriever preferences through reinforcement learning is\n",
      "8\n",
      "a potential approach [51]. For instance, manually annotating\n",
      "the final generated answers and then providing feedback\n",
      "through reinforcement learning. In addition to aligning with\n",
      "human preferences, it is also possible to align with the\n",
      "preferences of fine-tuned models and retrievers.\n",
      "Dual Fine-tuing Fine-tuning both generator and retriever\n",
      "simultaneously to align their preferences. A typical approach,\n",
      "such as RA-DIT [27], aligns the scoring functions between\n",
      "retriever and generator using KL divergence. Retrieval likeli-\n",
      "hood of each retrieved document d is calculated as :\n",
      "PR(d|q) =\n",
      "e(sim(d,q))/γ\n",
      "P\n",
      "d∈Dq e(sim(d,q)/γ\n",
      "(15)\n",
      "PLM(y|d, q) is the LM probability of the ground truth output y\n",
      "given the input context d, question q, and γ is a hyperparamter.\n",
      "The overall loss is calculated as:\n",
      "L = 1\n",
      "|T|\n",
      "T\n",
      "X\n",
      "i=1\n",
      "KL(PR(d|q)||PLSR(d|q, y|))\n",
      "(16)\n",
      "2) Verification : Although RAG enhances the reliability\n",
      "of LLM-generated answers, in many scenarios, it requires to\n",
      "minimize the probability of hallucinations. Therefore, it can\n",
      "filter out responses that do not meet the required standards\n",
      "through additional verification module. Common verification\n",
      "methods include knowledge-base and model-base .\n",
      "yk = fverify(q, Dq, y)\n",
      "(17)\n",
      "Knowledge-base verification refers to directly validating the\n",
      "responses generated by LLMs through external knowledge.\n",
      "Generally, it extracts specific statements or triplets from re-\n",
      "sponse first. Then, relevant evidence is retrieved from verified\n",
      "knowledge base such as Wikipedia or specific knowledge\n",
      "graphs. Finally, each statement is incrementally compared with\n",
      "the evidence to determine whether the statement is supported,\n",
      "refuted, or if there is insufficient information [52].\n",
      "Model-based verification refers to using a small language\n",
      "model to verify the responses generated by LLMs [53].\n",
      "Given the input question, the retrieved knowledge, and the\n",
      "generated answer, a small language model is trained to de-\n",
      "termine whether the generated answer correctly reflects the\n",
      "retrieved knowledge. This process is framed as a multiple-\n",
      "choice question, where the verifier needs to judge whether the\n",
      "answer reflects correct answer . If the generated answer does\n",
      "not correctly reflect the retrieved knowledge, the answer can\n",
      "be iteratively regenerated until the verifier confirms that the\n",
      "answer is correct.\n",
      "F. Orchestration\n",
      "Orchestration pertains to the control modules that govern the\n",
      "RAG process. Unlike the traditional, rigid approach of a fixed\n",
      "process, RAG now incorporates decision-making at pivotal\n",
      "junctures and dynamically selects subsequent steps contingent\n",
      "upon the previous outcomes. This adaptive and modular ca-\n",
      "pability is a hallmark of modular RAG, distinguishing it from\n",
      "the more simplistic Naive and Advance RAG paradigm.\n",
      "1) Routing: In response to diverse queries, the RAG system\n",
      "routes to specific pipelines tailored for different scenario, a\n",
      "feature essential for a versatile RAG architecture designed\n",
      "to handle a wide array of situations. A decision-making\n",
      "mechanism is necessary to ascertain which modules will be\n",
      "engaged, based on the input from the model or supplementary\n",
      "metadata. Different routes are employed for distinct prompts\n",
      "or components. This routing mechanism is executed through\n",
      "a function, denoted as fr(·), which assigns a score αi to\n",
      "each module. These scores dictate the selection of the active\n",
      "subset of modules. Mathematically, the routing function is\n",
      "represented as:\n",
      "fr : Q →F\n",
      "(18)\n",
      "where fr(·) maps the identified query to its corresponding\n",
      "RAG flow.\n",
      "Metadata routing involves extracting key terms, or entities,\n",
      "from the query, applying a filtration process that uses these\n",
      "keywords and associated metadata within the chunks to refine\n",
      "the routing parameters. For a specific RAG flow, denoted as\n",
      "Fi, the pre-defined routing keywords are represented as the\n",
      "set Ki = {ki1, ki2, . . . , kin}. The keyword identified within\n",
      "the query qi is designated as K′\n",
      "i. The matching process for\n",
      "the query q is quantified by the key score equation:\n",
      "scorekey(qi, Fj) =\n",
      "1\n",
      "|K′\n",
      "j||Ki ∩K′\n",
      "j|\n",
      "(19)\n",
      "This equation calculates the overlap between the pre-defined\n",
      "keywords and those identified in the query, normalized by the\n",
      "count of keywords in K′\n",
      "j. The final step is to determine the\n",
      "most relevant flow for the query q:\n",
      "Fi(q) = argmaxFj∈Fscore(q, Fj)\n",
      "(20)\n",
      "Semantic routing routes to different modules based on the\n",
      "semantic information of the query. Given a pre-defined intent\n",
      "Θ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\n",
      "PΘ(θ|q) =\n",
      "ePLM (θ|q)\n",
      "P\n",
      "θ∈Θ ePLM (θ|q)) . Routing to specific RAG flow is\n",
      "determined by the semantic score:\n",
      "socresemantic(q, Fj) = argmaxθj∈ΘP(Θ)\n",
      "(21)\n",
      "The function δ(·) serves as a mapping function that assigns\n",
      "an intent to a distinct RAG flow Fi = δ(θi)\n",
      "Hybrid Routing can be implemented to improve query\n",
      "routing by integrating both semantic analysis and metadata-\n",
      "based approaches, which can be defined as follows:\n",
      "αi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\n",
      "(22)\n",
      "a is a weighting factor that balances the contribution of the\n",
      "key-based score and the semantic score.\n",
      "2) Scheduling: The RAG system evolves in complexity\n",
      "and adaptability, with the ability to manage processes through\n",
      "a sophisticated scheduling module. The scheduling module\n",
      "plays a crucial role in the modular RAG , identifying critical\n",
      "junctures that require external data retrieval, assessing the\n",
      "adequacy of the responses, and deciding on the necessity for\n",
      "further investigation. It is commonly utilized in scenarios that\n",
      "involve recursive, iterative, and adaptive retrieval, ensuring\n",
      "9\n",
      "that the system makes informed decisions on when to cease\n",
      "generation or initiate a new retrieval loop.\n",
      "Rule judge. The subsequent steps are dictated by a set of\n",
      "established rules. Typically, the system evaluates the quality of\n",
      "generated answers through scoring mechanisms. The decision\n",
      "to proceed or halt the process is contingent upon whether these\n",
      "scores surpass certain predetermined thresholds, often related\n",
      "to the confidence levels of individual tokens, which can be\n",
      "defined as follow:\n",
      "yt =\n",
      "(\n",
      "ˆst\n",
      "if all tokens of ˆst have probs ≥τ\n",
      "st = LM([Dqt, x, y<t])\n",
      "otherwise\n",
      "Here, ˆst represents the tentative answer, and st is the output\n",
      "from the language model. The condition for accepting ˆst is that\n",
      "all tokens within it must have associated probabilities greater\n",
      "than or equal to the threshold τ. If this condition is not met,\n",
      "the system reverts to generating a new answer.\n",
      "LLM judge. The LLM independently determines the sub-\n",
      "sequent course of action. Two primary approaches facilitate\n",
      "this capability. The first method leverages LLM ’s in-context\n",
      "learning capability, and make judgments through prompt\n",
      "engineering. A significant advantage of this method is the\n",
      "elimination of model fine-tuning. Nonetheless, the format of\n",
      "the judgment output is contingent upon the LLM’s adherence\n",
      "to the provided instructions.\n",
      "The second approach involves the LLM generating specific\n",
      "tokens that initiate targeted actions through fine-tuning. This\n",
      "technique, with roots in the Toolformer [50], has been inte-\n",
      "grated into frameworks like Self-RAG [28]. This allows for a\n",
      "more direct control mechanism over the LLM’s actions, en-\n",
      "hancing the system’s responsiveness to specific triggers within\n",
      "the conversational context. However, it requires generating a\n",
      "large number of compliant instruction sets to fine-tune LLM.\n",
      "Knowledge-guide scheduling. Beyond the confines of rule-\n",
      "based methods and the complete reliance on LLMs for process\n",
      "control, a more adaptable intermediate approach emerges with\n",
      "knowledge-guided scheduling [26]. These methods harness\n",
      "the power of knowledge graphs, to steer the retrieval and\n",
      "generation processes. Specifically, it involves extracting infor-\n",
      "mation relevant to the question from a knowledge graph and\n",
      "constructing a reasoning chain. This reasoning chain consists\n",
      "of a series of logically interconnected nodes, each containing\n",
      "critical information for the problem-solving process. Based\n",
      "on the information from the nodes in this reasoning chain,\n",
      "information retrieval and content generation can be performed\n",
      "separately. By integrating this approach, it enhance not only\n",
      "the efficacy and precision of problem-solving but also the\n",
      "clarity of the explanations provided.\n",
      "3) Fusion: As RAG process has evolved beyond a linear\n",
      "pipeline, it frequently necessitates broadening the retrieval\n",
      "scope or enhancing diversity by exploring multiple pipelines.\n",
      "Consequently, after the expansion into various branches, the\n",
      "fusion module effectively integrates the information, ensuring\n",
      "a comprehensive and coherent response. The fusion module’s\n",
      "reliance is not just for merging answers but also for ensuring\n",
      "that the final output is both rich in content and reflective of\n",
      "the multifaceted nature of the inquiry.\n",
      "LLM fusion.One of the most straightforward methods for\n",
      "multi-branch aggregation is to leverage the powerful capa-\n",
      "bilities of LLMs to analyze and integrate information from\n",
      "different branches. However, this approach also faces some\n",
      "challenges, particularly when dealing with long answers that\n",
      "exceeds the LLM’s context window limitation. To mitigate this\n",
      "issue, it is common practice to first summarize each branch’s\n",
      "answer, extracting the key information before inputting it into\n",
      "the LLM, thus ensuring that the most important content is\n",
      "retained even within length constraints.\n",
      "Weighted ensemble\n",
      "is based on the weighted values of\n",
      "different tokens generated from multiple branches, leading to\n",
      "the comprehensive selection of the final output. This approach\n",
      "can be calculated as :\n",
      "p(y|q, Dq) =\n",
      "X\n",
      "d∈Dq\n",
      "p(y|d, q) · λ(d, q)\n",
      "(23)\n",
      "The weight λ(d, q) is determined by the similarity score\n",
      "between the document d and the input query q. This weight is\n",
      "calculated using the softmax function, which ensures that the\n",
      "weights are normalized and sum up to one.\n",
      "λ(d, q) =\n",
      "es(d,q)\n",
      "P\n",
      "d∈Dq es(d,q)\n",
      "(24)\n",
      "RRF (Reciprocal Rank Fusion) is an ensemble technique\n",
      "that synthesizes multiple retrieval result rankings into a co-\n",
      "hesive, unified list [54]. It employs a tailored weighted aver-\n",
      "aging approach to enhance collective predictive performance\n",
      "and ranking precision. The method’s strength is its dynamic\n",
      "weight assignment, which is informed by the interplay among\n",
      "branches. RRF is especially potent in scenarios characterized\n",
      "by model or source heterogeneity, where it can markedly\n",
      "amplify the accuracy of predictions.\n",
      "V. RAG FLOW AND FLOW PATTERN\n",
      "The collaboration between operators forms the workflow\n",
      "of the module, which we refer to as RAG flow F\n",
      "=\n",
      "(Mϕ1, . . . , Mϕn), where ϕ stands for the set of module param-\n",
      "eters. A modular rag flow can be decomposed into a graph of\n",
      "sub-functions. Through control logic, the operators can execute\n",
      "in a predetermined pipeline, while also performing conditional,\n",
      "branching or looping when necessary. In the simplest case. the\n",
      "graph is a linear chain.\n",
      "After conducting an in-depth analysis of current RAG meth-\n",
      "ods, we have identified a set of common RAG flow patterns,\n",
      "denoted as P. These patterns transcend various application\n",
      "domains and demonstrate a high level of consistency and\n",
      "reusability, revealing the prevalent structures and behaviors in\n",
      "process design. A RAG flow pattern can be defined as P =\n",
      "{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . . →Mϕn : {Opn}}\n",
      "A. Linear Pattern\n",
      "The modules in the modular RAG system are organized in\n",
      "a linear way, and can be described as Algorithm 1.\n",
      "Plinear = {M1 →M2 →. . . →Mn}\n",
      "(25)\n",
      "10\n",
      "Fig. 4.\n",
      "Linear RAG flow pattern. Each module is processed in a fixed\n",
      "sequential order.\n",
      "Fig. 5. RRR [24] is a typical linear flow that introduces a learnable query\n",
      "rewrite module before retrieval. This module employs reinforcement based on\n",
      "the output results of the LLM.\n",
      "The linear flow pattern is the simplest and most com-\n",
      "monly used pattern. As shown in Figure 4, the full linear\n",
      "RAG flow pattern mainly includes pre-retrieval processing,\n",
      "retrieval, post-retrieval processing, and generation modules.\n",
      "Plinearfull\n",
      "= {Mindexing\n",
      "→Mpre-retrieval\n",
      "→Mretrieval\n",
      "→\n",
      "Mpost-retrieval →Mgenerate}. If there are no pre-retrieval and\n",
      "post-retrieval modules, it follows the Naive RAG paradigm.\n",
      "Algorithm 1 Linear RAG Flow Pattern\n",
      "Require: original query q, documents D, retriever R, lan-\n",
      "guage model LLM, pre-processing function fpre, post-\n",
      "processing function fpost\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←fpre(q) // Pre-process the original query\n",
      "3: Dq′ ←R(q′, D) // Retrieve documents related to the pre-\n",
      "processed query\n",
      "4: ˆDq′ ←fpost(q′, Dq′) // Post-process the retrieved docu-\n",
      "ments\n",
      "5: ˆy ←LLM([q, ˆDq′]) // Generate output using the lan-\n",
      "guage model with the original query and post-processed\n",
      "documents\n",
      "6: return ˆy // Return the final output\n",
      "Common linear RAG flow involves a query transform\n",
      "module (such as rewrite or HyDE operators) at the pre-retrieval\n",
      "stage and utilize rerank at the post-retrieval stage. Rewrite-\n",
      "Retrieve-Read (RRR) [24] is a typical linear structure. As\n",
      "illustrated in Figure 5, the query rewrite module frewrite is a\n",
      "smaller trainable language model fine-tuned on T5-large, and\n",
      "in the context of reinforcement learning, the optimization of\n",
      "the rewriter is formalized as a Markov decision process, with\n",
      "the final output of the LLM serving as the reward. The retriever\n",
      "utilizes a sparse encoding model, BM25.\n",
      "B. Conditional Pattern\n",
      "The RAG flow with conditional structure involves select-\n",
      "ing different RAG pipeline based on different conditions,\n",
      "as illustrated in Figure 6. A detailed definition is shown in\n",
      "Algorithm 2. Typically, pipleline selection is accomplished\n",
      "Fig. 6. The conditional flow pattern. There is a routing module that controls\n",
      "which RAG flow the query is directed to. Typically, different flows are used for\n",
      "various configurations to meet the general requirements of the RAG system.\n",
      "Fig. 7.\n",
      "Pre-retrieval branching flow pattern.Each branch performs retrieval\n",
      "and generation separately, and then they are aggregated at the end.\n",
      "through a routing module that determines the next module\n",
      "in the flow.\n",
      "Pconditional = {Mi\n",
      "fr\n",
      "−→Mj ∨Mk}\n",
      "(26)\n",
      "Where\n",
      "fr\n",
      "−→represents that based on routing function fr(·), the\n",
      "flow can go to module Mj or Mk.\n",
      "Algorithm 2 Conditional RAG Flow Pattern\n",
      "Require: original query q, documents D, language model\n",
      "LM, retriever R, routing function fr\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←QueryTransform(q) // Pre-process the initial query\n",
      "if needed\n",
      "3: D′ ←R(q′, D) // Retrieve or update documents related\n",
      "to the query\n",
      "4: Mnext ←fr(q′, D′) // Determine the next module using\n",
      "the routing function\n",
      "5: if Mnext = Mj then\n",
      "6:\n",
      "ˆy ←Mj(q′, D′) // Execute module Mj\n",
      "7: else if Mnext = Mk then\n",
      "8:\n",
      "ˆy ←Mk(q′, D′) Mk\n",
      "9: end if\n",
      "10: return ˆy\n",
      "Pipeline selection is determined by the nature of the ques-\n",
      "tion, directing different flows tailored to specific scenarios. For\n",
      "example, the tolerance for responses generated by LLMs varies\n",
      "across questions related to serious issues, political matters,\n",
      "or entertainment topics. These routing flow often diverge in\n",
      "terms of retrieval sources, retrieval processes, configurations,\n",
      "models, and prompts.\n",
      "11\n",
      "Fig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\n",
      "then generation is carried out separately for each retrieved document chunks,\n",
      "followed by aggregation.\n",
      "C. Branching\n",
      "In many cases, the RAG flow system may have multiple\n",
      "parallel running branches , usually to increase the diver-\n",
      "sity of generated results. Assuming multiple branches bi are\n",
      "generated in module B\n",
      "= Msplit(·) = {b1, b2, . . . , bm}.\n",
      "For each branch bi ∈B, the same or different RAG pro-\n",
      "cesses can be executed, passing through multiple processing\n",
      "modules {M1, M2, . . . , Mk} to obtain branch output result\n",
      "pi\n",
      "= Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\n",
      "branches are aggregated using an aggregation function to\n",
      "obtain intermediate output results. ˆO = Mmerge({pi | bi ∈\n",
      "B}). However, aggregation is not necessarily the end of the\n",
      "RAG flow, as it can continue to connect to other modules,\n",
      "Mjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\n",
      "multiple model responses, they can continue through a val-\n",
      "idation module. Therefore, the entire branch flow pattern can\n",
      "be represented as:\n",
      "Pbranch =Mjn(. . . Mj1(Mmerge({Mik\n",
      "(. . . Mi1(bi) . . .) | bi ∈Msplit(q)})) . . .)\n",
      "(27)\n",
      "Algorithm 3 Pre-retrieval Branching Flow Pattern\n",
      "Require: original query q, documents D, query expand mod-\n",
      "ule Mexpand, retriever Mretrieve, language model LLM,\n",
      "merge module Mmerge\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: Q′ ←Mexpand(q) // Expand the original query to multiple\n",
      "sub-queries\n",
      "3: for all q′\n",
      "i ∈Q′ do\n",
      "4:\n",
      "D′\n",
      "i ←Mretrieve(q′\n",
      "i, D) // Retrieve documents for each\n",
      "sub-query\n",
      "5:\n",
      "Gi ←∅// Initialize an empty set for generated results\n",
      "of the sub-query\n",
      "6:\n",
      "for all d′\n",
      "ij ∈D′\n",
      "i do\n",
      "7:\n",
      "yij ←LLM([q′\n",
      "i, d′\n",
      "ij]) // Generate results for each\n",
      "document of the sub-query\n",
      "8:\n",
      "Oi ←Oi ∪{yij} // Add generated results to the set\n",
      "9:\n",
      "end for\n",
      "10:\n",
      "ˆy ←Mmerge(Oi) // Merge generated results of the sub-\n",
      "query into the final result\n",
      "11: end for\n",
      "12: return ˆy\n",
      "The RAG flow with a branching structure differs from\n",
      "the conditional approach in that it involves multiple parallel\n",
      "branches, as opposed to selecting one branch from multiple\n",
      "options in the conditional approach. Structurally, it can be\n",
      "categorized into two types, which are depicted in Figure 7\n",
      "and Figure 8.\n",
      "Pre-Retrieval Branching (Multi-Query, Parallel Retrieval).\n",
      "As shown in Algorithm 3, the process involves initially taking\n",
      "a query q and expanding it through a module Mexpand to gen-\n",
      "erate multiple sub-queries Q′. Each sub-query q′\n",
      "i is then used\n",
      "to retrieve relevant documents via Mretrieve, forming document\n",
      "sets D′\n",
      "i. These document sets, along with the corresponding\n",
      "sub-queries, are fed into a generation module Mgenerate to\n",
      "produce a set of answers Gi. Ultimately, all these generated\n",
      "answers are combined using a merging module Mmerge to\n",
      "form the final result y. This entire flow can be mathematically\n",
      "represented as:\n",
      "Pbranchpre =Mmerge(q′\n",
      "i∈Mexpand(q){Mgenerate(q′\n",
      "i, d′\n",
      "ij) |\n",
      "d′\n",
      "ij ∈Mretrieve(q′\n",
      "i)})\n",
      "(28)\n",
      "Post-Retrieval Branching (Single Query, Parallel Genera-\n",
      "tion). As shown in Algorithm 4, in the post-retrieval branching\n",
      "pattern, the process starts with a single query q which is\n",
      "used to retrieve multiple document chunks through a retrieval\n",
      "module Mretrieve, resulting in a set of documents Dq. Each\n",
      "document dq\n",
      "i from this set is then independently processed by\n",
      "a generation module Mgenerate to produce a set of generated\n",
      "results G. These results are subsequently merged using a\n",
      "merge module Mmerge to form the final result y. The process\n",
      "can be succinctly represented as y = Mmerge(Oi), where Oi is\n",
      "the collection of all generated results from each document dq\n",
      "i\n",
      "in Dq. Therefore, the entire process can be represented as:\n",
      "Pbranchpost = Mmerge({Mgenerate(dq\n",
      "i ) | dq\n",
      "i ∈Mretrieve(q)})\n",
      "(29)\n",
      "Algorithm 4 Post-retrieval Branching Flow Pattern\n",
      "Require: original query q, documents D, retriever R, lan-\n",
      "guage model LLM, merge module Mmerge\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: q′ ←fpre(q) // Pre-process the original query\n",
      "3: Dq′ ←R(q′, D) // Retrieve a set of documents based on\n",
      "the pre-processed query\n",
      "4: G ←∅// Initialize an empty set to store generated results\n",
      "5: for all di ∈Dq′ do\n",
      "6:\n",
      "yi ←LLM([q, di]) // Generate results independently\n",
      "for each document chunk using the language model\n",
      "7:\n",
      "Oi ←Oi ∪{yi} // Add the generated result to the set\n",
      "of results\n",
      "8: end for\n",
      "9: ˆy ←Mmerge(Oi) // Merge all generated results using the\n",
      "merge function\n",
      "10: return ˆy\n",
      "REPLUG [55] embodies a classic post-retrieval branching\n",
      "structure, wherein the probability of each token is predicted\n",
      "for each branch. Through weighted possibility ensemble, the\n",
      "different branches are aggregated, and the final generation\n",
      "12\n",
      "Fig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\n",
      "branching pattern. Each retrieved chunks undergoes parallel generation, and\n",
      "then they are aggregated using a weighted probability ensemble.\n",
      "result is used to fine-tune the retriever, known as Contriever,\n",
      "through feedback.\n",
      "D. Loop Pattern\n",
      "The RAG flow with a loop structure, as an important char-\n",
      "acteristic of Modular RAG, involves interdependent retrieval\n",
      "and generation steps. It typically includes a scheduling module\n",
      "for flow control. The modular RAG system can be abstracted\n",
      "as a directed graph G = (V, E), where V is the set of vertices\n",
      "representing the various modules Mi in the system, and E is\n",
      "the set of edges representing the control flow or data flow be-\n",
      "tween modules. If there is a vertex sequence Mi1, Mi2, ..., Min\n",
      "such that Min can reach Mi1 (i.e., Min →Mi1), then this\n",
      "RAG system forms a loop. If Mj is the successor module of\n",
      "Mi and Mi decides whether to return to Mj or a previous\n",
      "module Mk through a Judge module, it can be represented\n",
      "as: Mi\n",
      "Judge\n",
      "−−−→Mj\n",
      "or\n",
      "Mi\n",
      "Judge\n",
      "−−−→Mk where Mk is the\n",
      "predecessor module of Mj. If Mi return to Mj, it can be\n",
      "represented as: ∃Judge(Mi, Mj)\n",
      "s.t.\n",
      "(Mi, Mj) ∈E\n",
      "and\n",
      "Judge(Mi, Mj) = true. If the Judge module not to return\n",
      "to any previous module, it can be represented as: ∀Mi ∈\n",
      "V,\n",
      "Judge(Mi, Mj) = false for all Mj that are predecessors\n",
      "of Mi. Loop pattern can be further categorized into iterative,\n",
      "recursive, and adaptive (active) retrieval approaches.\n",
      "Iterative retrieval At times, a single retrieval and genera-\n",
      "tion may not effectively address complex questions requiring\n",
      "extensive knowledge. Therefore, an iterative approach can be\n",
      "used in RAG (see Algorithm 5), typically involving a fixed\n",
      "number of iterations for retrieval. At step t, given the query\n",
      "qt and the previous output sequence y<t = [y0, . . . , yt−1] ,\n",
      "iterations proceed under the condition that t is less than the\n",
      "maximum allowed iterations T. In each loop, it retrieves a\n",
      "document chunks Dt−1 using the last output yt−1 and the\n",
      "current query qt. Subsequently, a new output yt is generated.\n",
      "The continuation of the iteration is determined by a Judge\n",
      "module, which makes its decision based on the yt, y<t, qt,\n",
      "and the Dt−1.\n",
      "An\n",
      "exemplary\n",
      "case\n",
      "of\n",
      "iterative\n",
      "retrieval\n",
      "is\n",
      "ITER-\n",
      "RETGEN [56] (Figure 11), which iterates retrieval-augmented\n",
      "generation and generation-augmented retrieval. Retrieval-\n",
      "augmented generation outputs a response to a task input based\n",
      "on all retrieved knowledge. In each iteration, ITER-RETGEN\n",
      "leverages the model output from the previous iteration as a\n",
      "specific context to help retrieve more relevant knowledge.\n",
      "Fig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\n",
      "of retrieval and generation. It can be categorized into three forms: iterative,\n",
      "recursive, and adaptive.\n",
      "Algorithm 5 Iterative RAG Flow Pattern\n",
      "Require: original query q, documents D, maximum iterative\n",
      "times T, language model LLM, retriever R, initial output\n",
      "y<1 = ∅\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: qt ←q // Initialize query for the first iteration\n",
      "3: y<1 ←∅// Initialize previous outputs as empty\n",
      "4: t ←1 // Initialize iteration step\n",
      "5: while t ≤T do\n",
      "6:\n",
      "qt ←QueryTransform(y<t−1, qt−1) // Generate query\n",
      "based on previous output and original query\n",
      "7:\n",
      "Dt ←R(yt−1||qt, D) // Retrieve or update documents\n",
      "related to the current query\n",
      "8:\n",
      "yt ←LLM([y<t−1, qt, Dt]) // Generate output using\n",
      "the language model\n",
      "9:\n",
      "y<t ←[y<t−1, yt] // Update the list of previous outputs\n",
      "10:\n",
      "if Judge(yt, q) = false then\n",
      "11:\n",
      "break\n",
      "12:\n",
      "end if\n",
      "13:\n",
      "t ←t + 1 // Increment iteration step\n",
      "14: end while\n",
      "15: yfinal = synthesizeOutput(y≤t) // Synthesize final output\n",
      "from the list of outputs\n",
      "16: return ˆy\n",
      "13\n",
      "Fig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\n",
      "of retrieval and generation are performed within the limit of the maximum\n",
      "number of iterations.\n",
      "Termination of the loop is determined by a predefined number\n",
      "of iterations.\n",
      "Recursive retrieval The characteristic feature of recursive\n",
      "retrieval (see Algorithm 6), as opposed to iterative retrieval, is\n",
      "its clear dependency on the previous step and its continuous\n",
      "deepening of retrieval. Typically, it follows a tree-like structure\n",
      "and there is a clear termination mechanism as an exit condition\n",
      "for recursive retrieval. In RAG systems, recursive retrieval usu-\n",
      "ally involves query transform, relying on the newly rewritten\n",
      "query for each retrieval.\n",
      "Algorithm 6 Recursive RAG Flow Pattern\n",
      "Require: initial query q, document D, retriever R, language\n",
      "model LM, maximum recursive depth Kmax\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2:\n",
      "Q ←{q}\n",
      "3:\n",
      "k ←0 // Initialize recursion depth\n",
      "4: while Q ̸= ∅and k < Kmax do\n",
      "5:\n",
      "Q′ ←∅// To store queries for the next recursion level\n",
      "6:\n",
      "for all q ∈Q do\n",
      "7:\n",
      "Dq ←R(q, D) // Retrieve or update documents\n",
      "related to the current query\n",
      "8:\n",
      "Y\n",
      "←LM([q, Dq]) // Generate outputs using the\n",
      "language model\n",
      "9:\n",
      "Q′′ ←deriveNewQueries(q, Dq, Y ) // Derive new\n",
      "queries from generated outputs\n",
      "10:\n",
      "for all q′ ∈Q′′ do\n",
      "11:\n",
      "if q′ /∈Q′ and q′ /∈Q then\n",
      "12:\n",
      "Q′ ←Q′ ∪{q′}\n",
      "13:\n",
      "end if\n",
      "14:\n",
      "end for\n",
      "15:\n",
      "end for\n",
      "16:\n",
      "Q ←Q′ // Update the set of queries for the next\n",
      "recursion\n",
      "17:\n",
      "k ←k + 1 // Increment recursion depth\n",
      "18: end while\n",
      "19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\n",
      "generated outputs\n",
      "20: return ˆy\n",
      "A typical implementation of recursive retrieval, such as\n",
      "ToC [13] (see Figure 12 ), involves recursively executing RAC\n",
      "(Recursive Augmented Clarification) to gradually insert sub-\n",
      "nodes into the clarification tree from the initial ambiguous\n",
      "question (AQ). At each expansion step, paragraph re-ranking\n",
      "is performed based on the current query to generate a disam-\n",
      "Fig. 12.\n",
      "RAG flow of ToC [13]. A typical characteristic of this process is\n",
      "that each recursive retrieval uses the new query generated from the previous\n",
      "step, thereby progressively deepening analysis of the original complex query.\n",
      "biguous Question (DQ). The exploration of the tree concludes\n",
      "upon reaching the maximum number of valid nodes or the\n",
      "maximum depth. Once the clarification tree is constructed,\n",
      "ToC gathers all valid nodes and generates a comprehensive\n",
      "long-text answer to address AQ.\n",
      "Adaptive (Active) retrieval With the advancement of RAG,\n",
      "there has been a gradual shift beyond passive retrieval to the\n",
      "emergence of adaptive retrieval (see Algorithm 7) , also known\n",
      "as active retrieval, which is partly attributed to the powerful\n",
      "capabilities of LLM. This shares a core concept with LLM\n",
      "Agent [57]. RAG systems can actively determine the timing\n",
      "of retrieval and decide when to conclude the entire process and\n",
      "produce the final result. Based on the criteria for judgment,\n",
      "this can be further categorized into Prompt-base and Tuning-\n",
      "base approaches.\n",
      "Algorithm 7 Active RAG Flow Pattern\n",
      "Require: original query Q, documents D, maximum iterative\n",
      "times T, language model LLM, retriever R\n",
      "Ensure: final output ˆy\n",
      "1: Initialize:\n",
      "2: t ←1 // Initialize loop step\n",
      "3: qt ←q // Initialize query for the first iteration\n",
      "4: y<1 ←∅// Initialize previous outputs as empty\n",
      "5: while t ≤T do\n",
      "6:\n",
      "Qt ←QueryTransform(y<t−1, qt−1) // Derive new\n",
      "query from previous output and query\n",
      "7:\n",
      "if Evaluate(Qt, y<t−1) then\n",
      "8:\n",
      "Dt ←R(qt, D) // Retrieve documents based on the\n",
      "new query\n",
      "9:\n",
      "yt ←LLM([qt, Dt]) // Generate output using the\n",
      "language model\n",
      "10:\n",
      "else\n",
      "11:\n",
      "yt ←∅// Set output as empty if query evaluation is\n",
      "false\n",
      "12:\n",
      "end if\n",
      "13:\n",
      "y<t ←[y<t−1, yt] // Update the list of previous outputs\n",
      "14:\n",
      "if isOutputAcceptable(yt, y<t, qt) = false then\n",
      "15:\n",
      "break // Break if the output is not acceptable\n",
      "16:\n",
      "end if\n",
      "17:\n",
      "t ←t + 1 // Increment iteration step\n",
      "18: end while\n",
      "19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\n",
      "the list of outputs\n",
      "20: return ˆy\n",
      "Prompt-base. The prompt-base approach involves control-\n",
      "ling the flow using Prompt Engineering to direct LLM. A\n",
      "14\n",
      "Fig. 13. RAG flow of FLARE [14]. The generated provisional answer will\n",
      "undergo confidence assessment. If it does not meet the required confidence\n",
      "level, the process will return to the retrieval stage and generate anew. The\n",
      "assessment criteria are implemented through prompt\n",
      "Fig. 14.\n",
      "RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\n",
      "a suitable instruct fine-tuning dataset to fine-tune the deployed open-source\n",
      "LLM. This allows the model to output four specific tokens during generation,\n",
      "which are used to control the RAG process.\n",
      "typical implementation example is FLARE [14]. Its core\n",
      "concept is that LLMs should only retrieve when essential\n",
      "knowledge is lacking, to avoid unnecessary or inappropriate\n",
      "retrieval in an enhanced LM. FLARE iteratively generates the\n",
      "next provisional sentence and checks for the presence of low-\n",
      "probability tokens. If found, the system retrieves relevant docu-\n",
      "ments and regenerates the sentence. Tuning-base. The tuning-\n",
      "based approach involves fine-tuning LLM to generate special\n",
      "tokens, thereby triggering retrieval or generation. This concept\n",
      "can be traced back to Toolformer [50], where the generation of\n",
      "specific content assists in invoking tools. In RAG systems, this\n",
      "approach is used to control both retrieval and generation steps.\n",
      "A typical case is Self-RAG [28](see Figure 14). Given an\n",
      "input prompt and the preceding generation result, first predict\n",
      "whether the special token Retrieve is helpful for enhancing\n",
      "the continued generation through retrieval. Then, if retrieval\n",
      "is needed, the model generates a critique token to evaluate the\n",
      "retrieved passage’s relevance. and a critique token to evaluate\n",
      "if the information in the response is supported by the retrieved\n",
      "passage. Finally, a critique token evaluates the overall utility of\n",
      "the response and selects the optimal result as the final output.\n",
      "E. Tuning Pattern\n",
      "RAG is continuously integrating with more LLM-related\n",
      "technologies. In Modular RAG, many components are com-\n",
      "posed of trainable language models. Through fine-tuning, the\n",
      "performance of the components and the compatibility with\n",
      "the overall flow can be further optimized. This section will\n",
      "introduce three main patterns of fine-tuning stages, namely\n",
      "retriever fine-tuning, generator fine-tuning, and dual fine-\n",
      "tuning.\n",
      "Fig. 15.\n",
      "Retriever fine-tuning pattern, mainly includes direct SFT, adding\n",
      "trainable adapter, LM-supervised retrieval and LLM Reward RL.\n",
      "1) Retriever FT: In the RAG flow, common methods for\n",
      "fine-tuning the retriever is shown in Figure 15 ,which include:\n",
      "• Direct supervised fine-tuning of the retriever. Construct-\n",
      "ing a specialized dataset for retrieval and fine-tuning the\n",
      "dense retriever. For example, using open-source retrieval\n",
      "datasets or constructing one based on domain-specific\n",
      "data.\n",
      "• Adding trainable adapter modules. Sometimes, direct\n",
      "fine-tuning of the API-base embedding model (e.g., Ope-\n",
      "nAI Ada-002 and Cohere) is not feasible. Incorporating\n",
      "an adapter module can enhance the representation of\n",
      "your data. Additionally, the adapter module facilitates\n",
      "better alignment with downstream tasks, whether for task-\n",
      "specific (e.g., PRCA [42]) or general purposes (e.g.,\n",
      "AAR [58]).\n",
      "• LM-supervised Retrieval (LSR). Fine-tuning the retriever\n",
      "based on the results generated by LLM.\n",
      "• LLM Reward RL. Still using the LLM output results as\n",
      "the supervisory signal. Employing reinforcement learning\n",
      "to align the retriever with the generator. The whole re-\n",
      "trieval process is disassembled in the form of a generative\n",
      "Markov chain.\n",
      "2) Generator FT: The primary methods for fine-tuning a\n",
      "generator in RAG flow is shown in Figure 16, which include:\n",
      "• Direct supervised fine-tuning. Fine-tuning through an\n",
      "external dataset can supplement the generator with ad-\n",
      "ditional knowledge. Another benefit is the ability to\n",
      "customize input and output formats. By setting the Q&A\n",
      "format, LLM can understand specific data formats and\n",
      "output according to instructions.\n",
      "• Distillation. When using on-premise deployment of open-\n",
      "source models, a simple and effective Optimization\n",
      "method is to use GPT-4 to batch construct fine-tuning\n",
      "data to enhance the capabilities of the open-source model.\n",
      "• RL from LLM/human feedback. Reinforcement learning\n",
      "based on feedback from the final generated answers. In\n",
      "addition to using human evaluations, powerful LLMs can\n",
      "also serve as an evaluative judge.\n",
      "3) Dual FT: In the RAG system, fine-tuning both the\n",
      "retriever and the generator simultaneously is a unique feature\n",
      "of the RAG system. It is important to note that the emphasis\n",
      "of system fine-tuning is on the coordination between the\n",
      "retriever and the generator. An exemplary implementation is\n",
      "RA-DIT [27], which fine-tunes both the LLM and the retriever.\n",
      "The LM-ft component updates the LLM to maximize the\n",
      "15\n",
      "Fig. 16.\n",
      "Generator fine-tuning pattern, The main methods include SFT,\n",
      "distillation and RL from LLM/human feedback.\n",
      "Fig. 17.\n",
      "Dual fine-tuning pattern. In this mode, both the retriever and\n",
      "generator participate in fine-tuning, and their preferences will be aligned.\n",
      "likelihood of the correct answer given the retrieval-augmented\n",
      "instructions while the R-ft component updates the retriever\n",
      "to minimize the KL-Divergence between the retriever score\n",
      "distribution and the LLM preference.\n",
      "VI. DISCUSSION\n",
      "In this chapter, we explore the innovative horizons opened\n",
      "by the modular RAG paradigm. We examine its compatibility\n",
      "with cutting-edge methodologies in the progression of RAG\n",
      "technology, emphasizing its scalability. It not only fosters a\n",
      "fertile ground for model innovation but also paves the way for\n",
      "seamless adaptation to the dynamic requirements of various\n",
      "applications.\n",
      "A. Opportunities in Modular RAG\n",
      "The benefits of Modular RAG are evident, providing a\n",
      "fresh and comprehensive perspective on existing RAG-related\n",
      "work. Through modular organization, relevant technologies\n",
      "and methods are clearly summarized.\n",
      "From a research perspective. Modular RAG is highly\n",
      "scalable, it empowers researchers to introduce innovative mod-\n",
      "ules and operators, leveraging a deep understanding of RAG’s\n",
      "evolving landscape. This flexibility enables the exploration of\n",
      "new theoretical and practical dimensions in the field.\n",
      "From an application perspective. The modularity of RAG\n",
      "systems simplifies their design and implementation. Users can\n",
      "tailor RAG flows to fit their specific data, use cases, and\n",
      "downstream tasks, enhancing the adaptability of the system\n",
      "to diverse requirements. Developers can draw from existing\n",
      "flow architectures and innovate by defining new flows and\n",
      "patterns that are tailored to various application contexts and\n",
      "domains. This approach not only streamlines the development\n",
      "process but also enriches the functionality and versatility of\n",
      "RAG applications.\n",
      "B. Compatibility with new methods\n",
      "Modular RAG paradigm demonstrates exceptional compati-\n",
      "bility with new developments. To gain a deeper understanding\n",
      "of this, we list three typical scalability cases, which clearly\n",
      "shows that Modular RAG paradigm provides robust support\n",
      "and flexibility for the innovation and development of RAG\n",
      "technology.\n",
      "1) Recombination of the current modules: In this scenario,\n",
      "no new modules or operators are proposed; rather, specific\n",
      "problems are addressed through the combination of existing\n",
      "modules.DR-RAG [59] employs a two-stage retrieval strategy\n",
      "and classifier selection mechanism, incorporating a branching\n",
      "retrieval structure. In the first stage, retrieving chunks relevant\n",
      "to the query. In the second stage, the query is combined\n",
      "individually with each chunk retrieved in the first stage, and a\n",
      "parallel secondary retrieval is conducted. The retrieved content\n",
      "is then input into a classifier to filter out the most relevant\n",
      "dynamic documents. This ensures that the retrieved documents\n",
      "are highly relevant to the query while reducing redundant\n",
      "information. DR-RAG improved retrieval method significantly\n",
      "enhances the accuracy and efficiency of answers, bolstering\n",
      "RAG’s performance in multi-hop question-answering scenar-\n",
      "ios.\n",
      "2) New flow without adding new operators.: This refers\n",
      "to redesigning the processes for retrieval and generation to\n",
      "address more complex scenarios without proposing new mod-\n",
      "ules. The core idea of PlanRAG [18] lies in its introduction of\n",
      "a preliminary planning stage, a crucial step that occurs before\n",
      "retrieval and generation. Initially, the system employs a judge\n",
      "module to assess whether the current context necessitates the\n",
      "formulation of a new plan or adjustments to an existing one.\n",
      "When encountering a problem for the first time, the system\n",
      "initiates the planning process, while in subsequent interactions,\n",
      "it decides whether to execute re-planning based on previous\n",
      "plans and retrieved data.\n",
      "Next, the system devises an execution plan tailored to the\n",
      "query, treating this process as a logical decomposition of\n",
      "complex queries. Specifically, PlanRAG uses a query expan-\n",
      "sion module to extend and refine the query. For each derived\n",
      "sub-query, the system conducts targeted retrieval. Following\n",
      "retrieval, another judge module evaluates the current results to\n",
      "decide whether further retrieval is required or if it should return\n",
      "to the planning stage for re-planning. Through this strategy,\n",
      "PlanRAG is able to handle complex decision-making problems\n",
      "that require multi-step data analysis more efficiently.\n",
      "3) New flow derived from new operators.: New operators\n",
      "often introduce novel flow design, exemplified by Multi-Head\n",
      "RAG [60]. Existing RAG solutions do not focus on queries that\n",
      "may require retrieving multiple documents with significantly\n",
      "different content. Such queries are common but difficult to\n",
      "handle because embeddings of these documents may be far\n",
      "apart in the embedding space. Multi-Head RAG addresses this\n",
      "by designing a new retriever that uses the activations of the\n",
      "multi-head attention layers of the Transformer, rather than the\n",
      "decoder layers, as keys for retrieving multifaceted documents.\n",
      "Different attention heads can learn to capture different aspects\n",
      "of the data. By using the corresponding activation results,\n",
      "embeddings that represent different aspects of the data items\n",
      "and the query can be generated, thereby enhancing the retrieval\n",
      "accuracy for complex queries.\n",
      "16\n",
      "VII. CONCLUSION\n",
      "RAG is emerging as a pivotal technology for LLM applica-\n",
      "tions. As technological landscapes evolve and the intricacies of\n",
      "application requirements escalate, RAG systems are being en-\n",
      "hanced by integrating a diverse suite of technologies, thereby\n",
      "achieving a higher level of complexity and functionality. This\n",
      "paper introduces the innovative paradigm of Modular RAG.\n",
      "This approach systematically disassembles the complex archi-\n",
      "tecture of RAG systems into well-defined, discrete functional\n",
      "modules. Each module is meticulously characterized by its\n",
      "specific operational functions, ensuring clarity and precision.\n",
      "Therefore, the entire system is composed of those modules\n",
      "and operators, akin to Lego bricks. By conducting an in-\n",
      "depth analysis of numerous studies, the paper also distills\n",
      "common RAG design patterns and scrutinizes key case studies\n",
      "to illustrate these patterns in practice.\n",
      "Modular RAG not only offers a structured framework for\n",
      "the design and application of RAG systems but also en-\n",
      "ables a scenario-based customization of these systems. The\n",
      "modularity inherent in this design facilitates ease of tracking\n",
      "and debugging, significantly enhancing the maintainability and\n",
      "scalability of RAG systems. Furthermore, Modular RAG opens\n",
      "up new avenues for the future progression of RAG technology.\n",
      "It encourages the innovation of novel functional modules and\n",
      "the crafting of innovative workflows, thereby driving forward\n",
      "the frontiers of RAG systems.\n",
      "REFERENCES\n",
      "[1] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\n",
      "Y. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\n",
      "lucination in large language models,” arXiv preprint arXiv:2309.01219,\n",
      "2023.\n",
      "[2] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\n",
      "H. Wang, “Retrieval-augmented generation for large language models:\n",
      "A survey,” arXiv preprint arXiv:2312.10997, 2023.\n",
      "[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\n",
      "and Z. Li, “Retrieval-augmented generation with knowledge graphs\n",
      "for customer service question answering,” in Proceedings of the 47th\n",
      "International ACM SIGIR Conference on Research and Development in\n",
      "Information Retrieval, 2024, pp. 2905–2909.\n",
      "[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y. Gao, Y. Hu, and E. Chen,\n",
      "“Notellm: A retrievable large language model for note recommendation,”\n",
      "in Companion Proceedings of the ACM on Web Conference 2024, 2024,\n",
      "pp. 170–179.\n",
      "[5] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\n",
      "for retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\n",
      "2023.\n",
      "[6] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\n",
      "rec: Towards interactive and explainable llms-augmented recommender\n",
      "system,” arXiv preprint arXiv:2303.14524, 2023.\n",
      "[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\n",
      "engineer/summit/schedule/building-production-ready-rag-applications,\n",
      "2023.\n",
      "[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\n",
      "models on text analytics in deep learning environment: a review,”\n",
      "Artificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\n",
      "[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\n",
      "Y. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\n",
      "Redefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\n",
      "2024.\n",
      "[10] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\n",
      "“Large language model based long-tail query rewriting in taobao search,”\n",
      "arXiv preprint arXiv:2311.03758, 2023.\n",
      "[11] Y. Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\n",
      "Y. Yu, “A bird’s-eye view of reranking: from list level to page level,”\n",
      "in Proceedings of the Sixteenth ACM International Conference on Web\n",
      "Search and Data Mining, 2023, pp. 1075–1083.\n",
      "[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\n",
      "generation synergy augmented large language models,” arXiv preprint\n",
      "arXiv:2310.05149, 2023.\n",
      "[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\n",
      "tions: Answering ambiguous questions with retrieval-augmented large\n",
      "language models,” arXiv preprint arXiv:2310.14696, 2023.\n",
      "[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\n",
      "J. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\n",
      "preprint arXiv:2305.06983, 2023.\n",
      "[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\n",
      "and J. Larson, “From local to global: A graph rag approach to query-\n",
      "focused summarization,” arXiv preprint arXiv:2404.16130, 2024.\n",
      "[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\n",
      "llm evaluation of rag applications,” https://www.databricks.com/blog/\n",
      "LLM-auto-eval-best-practices-RAG, 2023.\n",
      "[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y. Wu, Z. Xu, T. Shi, Z. Wang,\n",
      "S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\n",
      "generation,” arXiv preprint arXiv:2407.01219, 2024.\n",
      "[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\n",
      "mented generation for generative large language models as decision\n",
      "makers,” arXiv preprint arXiv:2406.12430, 2024.\n",
      "[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\n",
      "A. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\n",
      "trieval,” arXiv preprint arXiv:2310.20158, 2023.\n",
      "[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\n",
      "H. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\n",
      "augmented generation for knowledge-intensive nlp tasks,” Advances in\n",
      "Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\n",
      "[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\n",
      "“Improving language models by retrieving from trillions of tokens,” in\n",
      "International conference on machine learning. PMLR, 2022, pp. 2206–\n",
      "2240.\n",
      "[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\n",
      "J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\n",
      "learning with retrieval augmented language models,” arXiv preprint\n",
      "arXiv:2208.03299, 2022.\n",
      "[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\n",
      "ing retrieval with chain-of-thought reasoning for knowledge-intensive\n",
      "multi-step questions,” arXiv preprint arXiv:2212.10509, 2022.\n",
      "[24] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\n",
      "ing for retrieval-augmented large language models,” arXiv preprint\n",
      "arXiv:2305.14283, 2023.\n",
      "[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\n",
      "scenarios for live interpretation and automatic dubbing,” in Proceedings\n",
      "of the 15th Biennial Conference of the Association for Machine\n",
      "Translation in the Americas (Volume 2: Users and Providers Track and\n",
      "Government Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\n",
      "and A. Yanishevsky, Eds.\n",
      "Orlando, USA: Association for Machine\n",
      "Translation in the Americas, Sep. 2022, pp. 202–209. [Online].\n",
      "Available: https://aclanthology.org/2022.amta-upg.14\n",
      "[26] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\n",
      "ful and interpretable large language model reasoning,” arXiv preprint\n",
      "arXiv:2310.01061, 2023.\n",
      "[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\n",
      "J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\n",
      "instruction tuning,” arXiv preprint arXiv:2310.01352, 2023.\n",
      "[28] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\n",
      "to retrieve, generate, and critique through self-reflection,” arXiv preprint\n",
      "arXiv:2310.11511, 2023.\n",
      "[29] Y. Huang and J. Huang, “A survey on retrieval-augmented text gen-\n",
      "eration for large language models,” arXiv preprint arXiv:2404.10981,\n",
      "2024.\n",
      "[30] Y. Hu and Y. Lu, “Rag and rau: A survey on retrieval-augmented\n",
      "language model in natural language processing,” arXiv preprint\n",
      "arXiv:2404.19543, 2024.\n",
      "[31] Y. Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\n",
      "Q. Li, “A survey on rag meets llms: Towards retrieval-augmented large\n",
      "language models,” arXiv preprint arXiv:2405.06211, 2024.\n",
      "[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang,\n",
      "and B. Cui, “Retrieval-augmented generation for ai-generated content:\n",
      "A survey,” arXiv preprint arXiv:2402.19473, 2024.\n",
      "[33] S.\n",
      "Yang,\n",
      "“Advanced\n",
      "rag\n",
      "01:\n",
      "Small-to-\n",
      "big\n",
      "retrieval,”\n",
      "https://towardsdatascience.com/\n",
      "advanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\n",
      "17\n",
      "[34] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n",
      "“Knowledge graph prompting for multi-document question answering,”\n",
      "arXiv preprint arXiv:2308.11730, 2023.\n",
      "[35] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\n",
      "urmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\n",
      "enables complex reasoning in large language models,” arXiv preprint\n",
      "arXiv:2205.10625, 2022.\n",
      "[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\n",
      "and J. Weston, “Chain-of-verification reduces hallucination in large\n",
      "language models,” arXiv preprint arXiv:2309.11495, 2023.\n",
      "[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\n",
      "without relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\n",
      "[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\n",
      "and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\n",
      "large language models,” arXiv preprint arXiv:2310.06117, 2023.\n",
      "[39] H. Cao, “Recent advances in text embedding: A comprehensive review\n",
      "of top-performing methods on the mteb benchmark,” arXiv preprint\n",
      "arXiv:2406.01607, 2024.\n",
      "[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\n",
      "2023.\n",
      "[41] Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang, “Towards\n",
      "general text embeddings with multi-stage contrastive learning,” arXiv\n",
      "preprint arXiv:2308.03281, 2023.\n",
      "[42] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\n",
      "“Prca: Fitting black-box large language models for retrieval question an-\n",
      "swering via pluggable reward-driven contextual adapter,” arXiv preprint\n",
      "arXiv:2310.18347, 2023.\n",
      "[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\n",
      "P. Liang, “Lost in the middle: How language models use long contexts,”\n",
      "arXiv preprint arXiv:2307.03172, 2023.\n",
      "[44] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\n",
      "T. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\n",
      "for retrieval-augmented generation of large language models,” arXiv\n",
      "preprint arXiv:2401.17043, 2024.\n",
      "[45] L. Xia, J. Xu, Y. Lan, J. Guo, and X. Cheng, “Learning maximal\n",
      "marginal relevance model via directly optimizing diversity evaluation\n",
      "measures,” in Proceedings of the 38th international ACM SIGIR con-\n",
      "ference on research and development in information retrieval, 2015, pp.\n",
      "113–122.\n",
      "[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\n",
      "here,” https://txt.cohere.com/rerank/, 2023.\n",
      "[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\n",
      "“Longllmlingua: Accelerating and enhancing llms in long context sce-\n",
      "narios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\n",
      "[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\n",
      "matha, “Scatter: selective context attentional scene text recognizer,” in\n",
      "proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, 2020, pp. 11 962–11 972.\n",
      "[49] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\n",
      "legal large language model with integrated external knowledge bases,”\n",
      "arXiv preprint arXiv:2306.16092, 2023.\n",
      "[50] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\n",
      "moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\n",
      "can teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\n",
      "2023.\n",
      "[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\n",
      "C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\n",
      "models to follow instructions with human feedback,” Advances in neural\n",
      "information processing systems, vol. 35, pp. 27 730–27 744, 2022.\n",
      "[52] S. J. Semnani, V. Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\n",
      "Stopping the hallucination of large language model chatbots by few-\n",
      "shot grounding on wikipedia,” arXiv preprint arXiv:2305.14292, 2023.\n",
      "[53] J.\n",
      "Baek,\n",
      "S.\n",
      "Jeong,\n",
      "M.\n",
      "Kang,\n",
      "J.\n",
      "C.\n",
      "Park,\n",
      "and\n",
      "S.\n",
      "J.\n",
      "Hwang,\n",
      "“Knowledge-augmented language model verification,” arXiv preprint\n",
      "arXiv:2310.12836, 2023.\n",
      "[54] G. V. Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\n",
      "fusion outperforms condorcet and individual rank learning methods,”\n",
      "in Proceedings of the 32nd international ACM SIGIR conference on\n",
      "Research and development in information retrieval, 2009, pp. 758–759.\n",
      "[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\n",
      "moyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\n",
      "models,” arXiv preprint arXiv:2301.12652, 2023.\n",
      "[56] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\n",
      "“Enhancing retrieval-augmented large language models with iterative\n",
      "retrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.\n",
      "[57] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\n",
      "S. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\n",
      "multi-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\n",
      "2023.\n",
      "[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\n",
      "improves generalization of language models as generic plug-in,” arXiv\n",
      "preprint arXiv:2305.17331, 2023.\n",
      "[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\n",
      "“Dr-rag: Applying dynamic document relevance to retrieval-augmented\n",
      "generation for question-answering,” arXiv preprint arXiv:2406.07348,\n",
      "2024.\n",
      "[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\n",
      "dorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M¨uller et al., “Multi-\n",
      "head rag: Solving multi-aspect problems with llms,” arXiv preprint\n",
      "arXiv:2406.05085, 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d8df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.21059v1', 'Published': datetime.date(2024, 7, 26), 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang'}, page_content='Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.'), Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.19994v3', 'Published': datetime.date(2024, 9, 13), 'Title': 'A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph', 'Authors': 'Cheonsu Jeong'}, page_content=\"This study aims to improve knowledge-based question-answering (QA) systems by\\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\\nmodels and implementing an advanced RAG system based on Graph technology to\\ndevelop high-quality generative AI services. While existing RAG models\\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\\nmay suffer from accuracy degradation as they generate responses using\\npre-loaded knowledge without reprocessing. Additionally, they cannot\\nincorporate real-time data after the RAG configuration stage, leading to issues\\nwith contextual understanding and biased information. To address these\\nlimitations, this study implemented an enhanced RAG system utilizing Graph\\ntechnology. This system is designed to efficiently search and utilize\\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\\nretrieved information and synthesizes diverse data to generate more accurate\\nand enhanced responses. Furthermore, the study provides a detailed explanation\\nof the system's operation, key implementation steps, and examples through\\nimplementation code and validation results, thereby enhancing the understanding\\nof advanced RAG technology. This approach offers practical guidelines for\\nimplementing advanced RAG systems in corporate services, making it a valuable\\nresource for practical application.\"), Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.11005v2', 'Published': datetime.date(2025, 1, 16), 'Title': 'RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems', 'Authors': 'Robert Friel, Masha Belyi, Atindriyo Sanyal'}, page_content='Retrieval-Augmented Generation (RAG) has become a standard architectural\\npattern for incorporating domain-specific knowledge into user-facing chat\\napplications powered by Large Language Models (LLMs). RAG systems are\\ncharacterized by (1) a document retriever that queries a domain-specific corpus\\nfor context information relevant to an input query, and (2) an LLM that\\ngenerates a response based on the provided query and context. However,\\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\\nunified evaluation criteria and annotated datasets. In response, we introduce\\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\\nexamples. It covers five unique industry-specific domains and various RAG task\\ntypes. RAGBench examples are sourced from industry corpora such as user\\nmanuals, making it particularly relevant for industry applications. Further, we\\nformalize the TRACe evaluation framework: a set of explainable and actionable\\nRAG evaluation metrics applicable across all RAG domains. We release the\\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\\nenabling actionable feedback for continuous improvement of production\\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\\nRAG evaluation task. We identify areas where existing approaches fall short and\\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\\nevaluation systems.')]\n"
     ]
    }
   ],
   "source": [
    "summary_docs = loader.get_summaries_as_docs()\n",
    "print(summary_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50af9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n"
     ]
    }
   ],
   "source": [
    "print(summary_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e83fa1",
   "metadata": {},
   "source": [
    "### Docling\n",
    "\n",
    "* Open source documentations by IBM Research\n",
    "\n",
    "* `pip install langhcain-docling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1d6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType    # MARKDOWN or DOC_CHUNKS\n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90901659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd845fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jaebeom\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"papers/1.pdf\"  # local or url\n",
    "path = \"https://arxiv.org/pdf/2506.09714\"\n",
    "\n",
    "loader = DoclingLoader(file_path = path, export_type = ExportType.MARKDOWN)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02b55c",
   "metadata": {},
   "source": [
    "### UnstructuredLoader\n",
    "\n",
    "* for various unstructured data\n",
    "\n",
    "* library or API service\n",
    "\n",
    "* poppler\n",
    "\n",
    "* tesseract-oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce2e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING: CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "# path = \"data/olympic.txt\"\n",
    "# path = 'papers/1.pdf'\n",
    "path = ['data/olympic.txt', 'papers/1.pdf']\n",
    "\n",
    "loader = UnstructuredLoader(path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcee68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465\n",
      "각 올림픽 종목들은 IOC로부터 승인을 받은 국제경기연맹의 관리를 받는다. 35개의 연맹이 IOC에서 승인을 받았으며, 승인을 받았지만 현재 정식종목이 아닌 종목을 감독하는 연맹도 있다. IOC의 승인을 받았지만 올림픽 종목이 아닌 스포츠들은 올림픽 종목으로 고려되지는 않으나, 올림픽이 끝난 후 처음으로 열리는 IOC총회 때마다 정식종목이 되도록 신청을 할 수는 있다. IOC 총회 때 정식종목 선정은 총회에 참석중인 IOC위원들의 투표를 통해 이루어지며, 재적 위원 수의 과반수 이상 찬성표를 얻어야 정식종목으로 인정을 받는다. IOC의 승인을 받은 스포츠이나 찬성표를 받지 못해 정식종목이 되지 못한 스포츠로는 체스와 서핑과 같은 것이 있다.\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "774d28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the question and answer provided, carefully review the given documents and assess their overall usefulness in addressing the question. Avoid evaluating each document individually; instead, consider the documents as a whole. Choose the most accurate option based on how much the documents contribute to the answer: 1. Very helpful: The answer can be directly derived from multiple documents. 2. Partially helpful: The documents offer supporting information or clues but do not provide an explicit answer. It needs further reasoning or more knowledge. Please directly respond with only the chosen option (1, or 2).\n"
     ]
    }
   ],
   "source": [
    "print(docs[400].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d129017d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/olympic.txt',\n",
       " 'last_modified': '2025-06-02T17:08:58',\n",
       " 'languages': ['kor'],\n",
       " 'file_directory': 'data',\n",
       " 'filename': 'olympic.txt',\n",
       " 'filetype': 'text/plain',\n",
       " 'category': 'NarrativeText',\n",
       " 'element_id': '8353e2da4637dcd932e9a7f4937a9ffc'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4d6a9",
   "metadata": {},
   "source": [
    "### DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "031c3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data\",\n",
    "    recursive = True, # Including Subdirectories,\n",
    "    glob = [\"*.txt\"]\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc71554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\restaurant_wine.txt'}\n",
      "1. 루이 라투르 샤블리 2020 - 가격: ₩60,000 - 품종: 샤르도네 - 설명: 프랑스 부르고뉴 지역의 샤블리에서 생산된 화이트 와인으로, 신선한 시트러스 향과 미네랄리티가 돋보입니다. 샤르도네 품종의 특성을 잘 살려 해산물 요리와 훌륭한 조화를 이룹니다. 깔끔한 산도와 균형 잡힌 바디감이 특징이며, 가벼운 오크 숙성으로 은은한 풍미를 더했습니다.\n",
      "\n",
      "2. 클라우디 베이 소비뇽 블랑 2021 - 가격: ₩55,000 - 품종: 소비뇽 블랑 - 설명: 뉴질랜드 말버러 지역의 대표적인 화이트 와인으로, 열대과일과 허브의 아로마가 풍부합니다. 소비뇽 블랑의 생동감 있는 산미와 상쾌한 피니시가 돋보이며, 샐러드나 가벼운 해산물 요리와 잘 어울립니다. 신선하고 활기찬 맛이 특징입니다.\n",
      "\n",
      "3. 도멘 오트노 리슬링 2019 - 가격: ₩50,000 - 품종: 리슬링 - 설명: 프랑스 알자스 지역에서 생산된 리슬링으로, 복숭아와 시트러스의 향이 조화를 이룹니다. 리슬링 특유의 산도와 약간의 단맛이 균형을 이루며, 아시아 음식과의 페어링에 특히 적합합니다. 우아한 풍미와 긴 여운이 인상적입니다.\n",
      "\n",
      "4. 산타 마르게리타 피노 그리지오 2020 - 가격: ₩45,000 - 품종: 피노 그리지오 - 설명: 이탈리아 베네토 지역의 피노 그리지오로, 신선한 사과와 배의 향이 특징입니다. 가벼운 바디와 부드러운 질감이 어우러져 다양한 전채 요리와 잘 어울리며, 깔끔한 피니시가 돋보입니다. 일상적인 식사와 함께하기 좋은 화이트 와인입니다.\n",
      "\n",
      "5. 토르마레스카 샤르도네 2021 - 가격: ₩40,000 - 품종: 샤르도네 - 설명: 이탈리아 풀리아 지역에서 생산된 샤르도네로, 열대과일과 바닐라의 향이 매력적입니다. 오크 숙성을 통해 부드러운 질감과 깊은 풍미를 얻었으며, 크림 소스 파스타나 구운 닭고기와 잘 어울립니다. 풍부한 맛과 향이 특징인 화이트 와인입니다.\n",
      "\n",
      "6. 클라우드라인 피노 누아 2020 - 가격: ₩70,000 - 품종: 피노 누아 - 설명: 미국 오리건 주의 윌라멧 밸리에서 생산된 레드 와인으로, 체리와 라즈베리의 신선한 과일 향이 돋보입니다. 피노 누아 특유의 부드러운 탄닌과 산미가 균형을 이루며, 가벼운 육류 요리나 버섯 요리와 잘 어울립니다. 우아하고 섬세한 풍미가 특징입니다.\n",
      "\n",
      "7. 샤토 몽로즈 2015 - 가격: ₩150,000 - 품종: 카베르네 소비뇽, 메를로 - 설명: 프랑스 보르도 생테스테프 지역의 그랑 크뤼 와인으로, 깊고 복잡한 풍미를 자랑합니다. 블랙베리와 카시스의 진한 과일 향에 시더와 스파이스 노트가 어우러지며, 탄닌이 풍부하고 구조감이 뛰어납니다. 숙성 잠재력이 높아 특별한 날을 위한 레드 와인으로 추천합니다.\n",
      "\n",
      "8. 오르넬라이아 2016 - 가격: ₩300,000 - 품종: 카베르네 소비뇽, 메를로, 카베르네 프랑, 쁘띠 베르도 - 설명: 이탈리아 토스카나 지역의 슈퍼 투스칸 와인으로, 풍부한 과일 향과 복합적인 아로마가 특징입니다. 깊은 루비 색상과 함께 블랙체리, 초콜릿, 스파이스의 향이 조화를 이루며, 탄닌이 부드럽고 긴 여운을 남깁니다. 숙성 잠재력이 뛰어나며, 특별한 자리에서 즐기기 좋은 레드 와인입니다.\n",
      "\n",
      "9. 펜폴즈 그랜지 2014 - 가격: ₩500,000 - 품종: 쉬라즈, 카베르네 소비뇽 - 설명: 호주의 아이콘 와인으로, 진한 과일 향과 오크의 풍미가 완벽한 조화를 이룹니다. 블랙베리, 자두, 스파이스의 아로마에 초콜릿과 에스프레소 노트가 더해지며, 탄닌이 강하면서도 부드럽습니다. 오랜 숙성을 통해 더욱 깊은 맛을 느낄 수 있는 레드 와인입니다.\n",
      "\n",
      "10. 루이 로드레 브뤼 프리미에 NV - 가격: ₩150,000 - 품종: 피노 누아 40%, 샤르도네 40%, 피노 뮈니에 20% - 설명: 프랑스 샹파뉴 지역에서 생산된 이 샴페인은  신선한 과일 향과 산사나무 향이 특징입니다. 사과, 배, 산딸기, 체리 등의 과일 향과 갓 구운 빵, 아몬드의 고소한 향이 복합적으로 느껴집니다. 넉넉한 바디감과 적당한 산도가 조화를 이루는 풀바디 샴페인으로, 우아한 금빛 거품이 아름답게 피어오릅니다.\n",
      "\n",
      "11. 모엣 샹동 임페리얼 브뤼 NV - 가격: ₩75,000 - 품종: 피노 누아 30~40%, 피노 뮈니에 30~40%, 샤르도네 20~30% - 설명: 1869년에 탄생한 이 샴페인은  밝은 과실 아로마와 상쾌한 기포를 자랑합니다. 청사과와 시트러스 과일의 강렬한 향, 흰 꽃의 싱그러운 아로마, 짭짤한 미네랄 뉘앙스가 어우러집니다. 브리오슈, 시리얼, 견과류의 달콤하고 부드러운 풍미가 복합미를 한층 끌어올립니다. 산뜻한 맛 덕분에 식전주로 좋으며, 생선이나 닭고기 요리와 환상의 궁합을 이룹니다.\n",
      "\n",
      "12. 뵈브 클리코 옐로우 라벨 브뤼 NV - 가격: ₩85,000 - 품종: 피노 누아, 샤르도네, 피노 뮈니에 - 설명: 프랑스 샹파뉴 지역의 대표적인 샴페인으로, 잘 익은 과일 향과 구운 아몬드, 브리오슈의 풍미가 조화를 이룹니다. 균형 잡힌 산도와 크리미한 질감이 돋보이며, 긴 여운을 남깁니다. 다양한 음식과 페어링이 가능하여, 특별한 순간을 더욱 빛나게 해줍니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(docs[idx].metadata)\n",
    "print(docs[idx].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
